[2022-08-13 13:14:27,993] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:68)
[2022-08-13 13:14:28,001] INFO WorkerInfo values: 
	jvm.args = -Xmx256M, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=C:\kafka/logs, -Dlog4j.configuration=file:C:\kafka/config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 17.0.2, 17.0.2+8-LTS-86
	jvm.classpath = C:\kafka\libs\activation-1.1.1.jar;C:\kafka\libs\aopalliance-repackaged-2.6.1.jar;C:\kafka\libs\argparse4j-0.7.0.jar;C:\kafka\libs\audience-annotations-0.5.0.jar;C:\kafka\libs\commons-cli-1.4.jar;C:\kafka\libs\commons-lang3-3.8.1.jar;C:\kafka\libs\connect-api-3.1.0.jar;C:\kafka\libs\connect-basic-auth-extension-3.1.0.jar;C:\kafka\libs\connect-file-3.1.0.jar;C:\kafka\libs\connect-json-3.1.0.jar;C:\kafka\libs\connect-mirror-3.1.0.jar;C:\kafka\libs\connect-mirror-client-3.1.0.jar;C:\kafka\libs\connect-runtime-3.1.0.jar;C:\kafka\libs\connect-transforms-3.1.0.jar;C:\kafka\libs\hk2-api-2.6.1.jar;C:\kafka\libs\hk2-locator-2.6.1.jar;C:\kafka\libs\hk2-utils-2.6.1.jar;C:\kafka\libs\jackson-annotations-2.12.3.jar;C:\kafka\libs\jackson-core-2.12.3.jar;C:\kafka\libs\jackson-databind-2.12.3.jar;C:\kafka\libs\jackson-dataformat-csv-2.12.3.jar;C:\kafka\libs\jackson-datatype-jdk8-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-base-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-json-provider-2.12.3.jar;C:\kafka\libs\jackson-module-jaxb-annotations-2.12.3.jar;C:\kafka\libs\jackson-module-scala_2.13-2.12.3.jar;C:\kafka\libs\jakarta.activation-api-1.2.1.jar;C:\kafka\libs\jakarta.annotation-api-1.3.5.jar;C:\kafka\libs\jakarta.inject-2.6.1.jar;C:\kafka\libs\jakarta.validation-api-2.0.2.jar;C:\kafka\libs\jakarta.ws.rs-api-2.1.6.jar;C:\kafka\libs\jakarta.xml.bind-api-2.3.2.jar;C:\kafka\libs\javassist-3.27.0-GA.jar;C:\kafka\libs\javax.servlet-api-3.1.0.jar;C:\kafka\libs\javax.ws.rs-api-2.1.1.jar;C:\kafka\libs\jaxb-api-2.3.0.jar;C:\kafka\libs\jersey-client-2.34.jar;C:\kafka\libs\jersey-common-2.34.jar;C:\kafka\libs\jersey-container-servlet-2.34.jar;C:\kafka\libs\jersey-container-servlet-core-2.34.jar;C:\kafka\libs\jersey-hk2-2.34.jar;C:\kafka\libs\jersey-server-2.34.jar;C:\kafka\libs\jetty-client-9.4.43.v20210629.jar;C:\kafka\libs\jetty-continuation-9.4.43.v20210629.jar;C:\kafka\libs\jetty-http-9.4.43.v20210629.jar;C:\kafka\libs\jetty-io-9.4.43.v20210629.jar;C:\kafka\libs\jetty-security-9.4.43.v20210629.jar;C:\kafka\libs\jetty-server-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlet-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlets-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-ajax-9.4.43.v20210629.jar;C:\kafka\libs\jline-3.12.1.jar;C:\kafka\libs\jopt-simple-5.0.4.jar;C:\kafka\libs\jose4j-0.7.8.jar;C:\kafka\libs\kafka-clients-3.1.0.jar;C:\kafka\libs\kafka-log4j-appender-3.1.0.jar;C:\kafka\libs\kafka-metadata-3.1.0.jar;C:\kafka\libs\kafka-raft-3.1.0.jar;C:\kafka\libs\kafka-server-common-3.1.0.jar;C:\kafka\libs\kafka-shell-3.1.0.jar;C:\kafka\libs\kafka-storage-3.1.0.jar;C:\kafka\libs\kafka-storage-api-3.1.0.jar;C:\kafka\libs\kafka-streams-3.1.0.jar;C:\kafka\libs\kafka-streams-examples-3.1.0.jar;C:\kafka\libs\kafka-streams-scala_2.13-3.1.0.jar;C:\kafka\libs\kafka-streams-test-utils-3.1.0.jar;C:\kafka\libs\kafka-tools-3.1.0.jar;C:\kafka\libs\kafka_2.13-3.1.0.jar;C:\kafka\libs\log4j-1.2.17.jar;C:\kafka\libs\lz4-java-1.8.0.jar;C:\kafka\libs\maven-artifact-3.8.1.jar;C:\kafka\libs\metrics-core-2.2.0.jar;C:\kafka\libs\metrics-core-4.1.12.1.jar;C:\kafka\libs\netty-buffer-4.1.68.Final.jar;C:\kafka\libs\netty-codec-4.1.68.Final.jar;C:\kafka\libs\netty-common-4.1.68.Final.jar;C:\kafka\libs\netty-handler-4.1.68.Final.jar;C:\kafka\libs\netty-resolver-4.1.68.Final.jar;C:\kafka\libs\netty-transport-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-epoll-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-unix-common-4.1.68.Final.jar;C:\kafka\libs\osgi-resource-locator-1.0.3.jar;C:\kafka\libs\paranamer-2.8.jar;C:\kafka\libs\plexus-utils-3.2.1.jar;C:\kafka\libs\reflections-0.9.12.jar;C:\kafka\libs\rocksdbjni-6.22.1.1.jar;C:\kafka\libs\scala-collection-compat_2.13-2.4.4.jar;C:\kafka\libs\scala-java8-compat_2.13-1.0.0.jar;C:\kafka\libs\scala-library-2.13.6.jar;C:\kafka\libs\scala-logging_2.13-3.9.3.jar;C:\kafka\libs\scala-reflect-2.13.6.jar;C:\kafka\libs\slf4j-api-1.7.30.jar;C:\kafka\libs\slf4j-log4j12-1.7.30.jar;C:\kafka\libs\snappy-java-1.1.8.4.jar;C:\kafka\libs\trogdor-3.1.0.jar;C:\kafka\libs\zookeeper-3.6.3.jar;C:\kafka\libs\zookeeper-jute-3.6.3.jar;C:\kafka\libs\zstd-jni-1.5.0-4.jar
	os.spec = Windows 10, amd64, 10.0
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2022-08-13 13:14:28,003] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:77)
[2022-08-13 13:14:28,014] INFO Loading plugin from: C:\kafka\opt\connectors\debezium-debezium-connector-mysql-1.9.3 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:265)
[2022-08-13 13:14:28,389] INFO Registered loader: PluginClassLoader{pluginLocation=file:/C:/kafka/opt/connectors/debezium-debezium-connector-mysql-1.9.3/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 13:14:28,390] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,392] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,392] INFO Added plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,392] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,392] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,392] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,393] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,393] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,393] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,393] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,393] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,395] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:28,395] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,126] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@266474c2 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 13:14:29,127] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,128] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,128] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,128] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,128] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,128] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,128] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,129] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,129] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,129] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,129] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,129] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,129] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,130] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,130] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,130] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,130] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,130] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,131] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,131] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,131] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,131] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,131] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,131] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,131] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,131] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,131] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,132] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,132] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,132] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,132] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,132] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,132] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,132] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,132] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,133] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,133] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,133] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,133] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,133] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,133] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,133] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,133] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,133] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,133] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,134] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,134] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,134] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,134] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,134] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,134] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,134] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,134] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:14:29,135] INFO Added aliases 'MySqlConnector' and 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,135] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,135] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,135] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,135] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,136] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,136] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,136] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,136] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,136] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,136] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,136] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,137] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,137] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,137] INFO Added aliases 'CloudEventsConverter' and 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,137] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,137] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,137] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,137] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,137] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,148] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,148] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,148] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,148] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,149] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,149] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,149] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,149] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,149] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,149] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,149] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,150] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,150] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,150] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,150] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,150] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,151] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,151] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,151] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,151] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,151] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,151] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,151] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,151] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,152] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,152] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,152] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,152] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:14:29,152] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,152] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,152] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:14:29,165] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = All
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = C:/kafka/tmp/connect.offsets
	plugin.path = [/opt/connectors, C:/kafka/opt/connectors]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:376)
[2022-08-13 13:14:29,166] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 13:14:29,169] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 13:14:29,240] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,240] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,242] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,242] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,242] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,242] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,242] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,243] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:14:29,243] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:14:29,243] INFO Kafka startTimeMs: 1660376669242 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:14:29,483] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 13:14:29,485] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:14:29,489] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:14:29,490] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:14:29,490] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:14:29,499] INFO Logging initialized @1847ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2022-08-13 13:14:29,525] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:117)
[2022-08-13 13:14:29,526] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:188)
[2022-08-13 13:14:29,531] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 17.0.2+8-LTS-86 (org.eclipse.jetty.server.Server:375)
[2022-08-13 13:14:29,551] INFO Started http_8083@42f22995{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2022-08-13 13:14:29,551] INFO Started @1899ms (org.eclipse.jetty.server.Server:415)
[2022-08-13 13:14:29,566] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 13:14:29,566] INFO REST server listening at http://192.168.0.111:8083/, advertising URL http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2022-08-13 13:14:29,566] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 13:14:29,566] INFO REST admin endpoints at http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2022-08-13 13:14:29,567] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 13:14:29,567] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:44)
[2022-08-13 13:14:29,571] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 13:14:29,572] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 13:14:29,577] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,584] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,584] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,584] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,584] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,585] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,585] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:29,585] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:14:29,585] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:14:29,585] INFO Kafka startTimeMs: 1660376669585 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:14:29,596] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 13:14:29,597] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:14:29,599] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:14:29,600] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:14:29,600] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:14:29,603] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:14:29,603] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:14:29,603] INFO Kafka startTimeMs: 1660376669603 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:14:29,684] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:14:29,686] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:14:29,693] INFO Kafka Connect standalone worker initialization took 1696ms (org.apache.kafka.connect.cli.ConnectStandalone:99)
[2022-08-13 13:14:29,693] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2022-08-13 13:14:29,693] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:98)
[2022-08-13 13:14:29,693] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:185)
[2022-08-13 13:14:29,694] INFO Starting FileOffsetBackingStore with file C:\kafka\tmp\connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2022-08-13 13:14:29,702] INFO Worker started (org.apache.kafka.connect.runtime.Worker:192)
[2022-08-13 13:14:29,703] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:101)
[2022-08-13 13:14:29,703] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:208)
[2022-08-13 13:14:29,734] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:225)
[2022-08-13 13:14:29,777] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2022-08-13 13:14:29,777] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2022-08-13 13:14:29,779] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2022-08-13 13:14:30,101] INFO Started o.e.j.s.ServletContextHandler@2cec704c{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:915)
[2022-08-13 13:14:30,102] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2022-08-13 13:14:30,102] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2022-08-13 13:14:30,275] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 13:14:30,280] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 13:14:30,282] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 13:14:30,290] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 13:14:30,291] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:14:30,292] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:14:30,295] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 13:14:30,296] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 13:14:30,298] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:14:30,299] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:14:30,301] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 13:14:30,309] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 13:14:30,310] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:14:30,312] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 13:14:30,313] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 13:14:30,314] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:14:30,314] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:14:30,314] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 13:14:30,314] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 13:14:30,315] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 13:14:30,317] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:14:30,317] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:14:30,326] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 13:14:30,331] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 13:14:30,352] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 13:14:30,353] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:14:30,354] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:14:30,354] INFO [connector1|task-0] Kafka startTimeMs: 1660376670353 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:14:30,362] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:14:30,363] INFO Created connector connector1 (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2022-08-13 13:14:30,363] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 13:14:30,364] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,364] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,365] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,365] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,365] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,365] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,365] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,365] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,366] INFO [connector1|task-0]    transforms.unwrap.add.source.fields = table (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,369] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,369] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,369] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,369] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,369] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,370] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,370] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,370] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,370] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,370] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,370] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,370] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,370] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:14:30,426] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1599608, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 13:14:30,450] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 13:14:30,451] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 13:14:30,451] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 13:14:30,452] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 13:14:30,457] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:14:30,457] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:14:30,457] INFO [connector1|task-0] Kafka startTimeMs: 1660376670457 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:14:30,463] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:14:30,476] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 13:14:30,477] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 13:14:30,483] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:14:30,507] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:14:30,507] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:14:30,508] INFO [connector1|task-0] Kafka startTimeMs: 1660376670507 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:14:30,515] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:14:30,521] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:14:30,522] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:14:30,522] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:14:30,522] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:14:30,522] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:14:30,523] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:14:30,524] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:14:30,528] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:14:30,533] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:14:30,534] INFO [connector1|task-0] Kafka startTimeMs: 1660376670528 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:14:30,534] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 13:14:30,535] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 13:14:30,541] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:30,541] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:30,550] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:30,550] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:30,551] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:30,549] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:14:30,551] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:30,551] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:14:30,551] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:14:30,551] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:14:30,551] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:14:30,551] INFO [connector1|task-0] Kafka startTimeMs: 1660376670551 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:14:30,575] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:14:30,581] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:14:30,582] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:14:30,582] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:14:30,582] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:14:30,583] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:14:30,584] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:14:30,590] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:14:30,593] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 13:14:30,597] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:14:30,598] INFO [connector1|task-0] Kafka startTimeMs: 1660376670590 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:14:30,598] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:14:30,600] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:14:30,600] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:14:30,600] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:14:30,603] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:14:30,606] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:14:30,608] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:14:30,609] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:14:30,609] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:14:30,610] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:14:30,611] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:14:30,612] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:14:30,617] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:14:30,618] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:14:30,618] INFO [connector1|task-0] Kafka startTimeMs: 1660376670617 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:14:30,622] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:14:30,628] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:14:30,633] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:14:30,633] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:14:30,634] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:14:30,634] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:14:30,634] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:14:30,635] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:14:30,635] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 13:14:30,642] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:14:30,646] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:14:30,646] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:14:30,646] INFO [connector1|task-0] Kafka startTimeMs: 1660376670646 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:14:30,647] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 13:14:30,650] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:14:30,650] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:14:30,658] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 13:14:30,660] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 13:14:30,674] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:14:30,674] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 13:14:30,687] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=1, memberId='order_server-dbhistory-925f1f4e-e983-4705-b9b2-bf8e245f28f1', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 13:14:30,688] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 1: {order_server-dbhistory-925f1f4e-e983-4705-b9b2-bf8e245f28f1=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 13:14:30,726] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=1, memberId='order_server-dbhistory-925f1f4e-e983-4705-b9b2-bf8e245f28f1', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 13:14:30,727] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 13:14:30,729] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 13:14:30,739] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 13:14:30,743] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 13:14:31,192] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 13:14:31,192] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-925f1f4e-e983-4705-b9b2-bf8e245f28f1 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 13:14:31,195] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:14:31,196] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:14:31,207] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:14:31,207] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:14:31,208] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:14:31,209] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:14:31,209] INFO [connector1|task-0] Finished database history recovery of 15 change(s) in 574 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 13:14:31,227] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 13:14:31,233] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 13:14:31,236] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 13:14:31,258] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 13:14:31,259] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 13:14:31,260] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 13:14:31,260] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 13:14:31,263] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 13:14:31,263] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 13:14:31,270] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 13:14:31,271] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1599608, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1599608, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 13:14:31,274] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 13:14:31,278] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 13:14:31,288] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 13:14:31,289] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 13:14:31,289] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:14:31,293] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:14:31,305] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1599608, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1599608, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 13:14:31,305] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 13:14:31,306] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:14:31,408] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 13:14:40,367] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:14:50,378] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:15:00,390] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:15:10,395] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:15:20,399] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:15:30,410] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:15:40,425] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:15:50,440] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:16:00,451] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:16:10,462] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:16:20,469] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:16:30,483] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:16:40,488] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:16:50,493] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:17:00,497] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:17:10,512] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:17:20,524] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:17:30,539] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:17:40,546] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:17:50,556] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:18:00,562] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:18:10,568] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:18:20,573] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:18:30,589] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:18:40,604] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:18:50,611] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:19:00,619] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:19:10,633] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:19:20,639] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:19:30,656] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:19:40,659] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:19:50,660] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:20:00,669] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:20:10,672] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:20:20,672] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:20:30,678] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:20:40,681] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:20:50,689] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:21:00,690] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:21:10,692] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:21:20,695] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:21:30,697] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:21:40,711] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:21:50,723] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:22:00,730] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:22:10,736] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:22:20,751] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:22:30,752] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:22:40,756] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:22:50,773] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:23:00,788] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:23:10,794] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:23:20,806] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:23:30,455] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 13:23:30,547] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 13:23:30,809] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:23:40,307] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:23:40,335] INFO [connector1|task-0] Already applied 16 database changes (io.debezium.relational.history.DatabaseHistoryMetrics:132)
[2022-08-13 13:23:40,668] INFO [connector1|task-0] 3 records sent during previous 00:09:10.354, last recorded offset: {transaction_id=null, ts_sec=1660377220, file=SF-CPU-562-bin.000076, pos=1602012, server_id=1} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 13:23:40,674] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server-0 to 0 since the associated topicId changed from null to LVkchKJTThukBrB7dOsgOA (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:23:40,823] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:23:50,836] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:23:52,842] INFO [connector1|task-0] Already applied 19 database changes (io.debezium.relational.history.DatabaseHistoryMetrics:132)
[2022-08-13 13:24:00,854] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:24:10,859] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:24:20,867] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:24:30,879] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:24:40,886] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:24:50,902] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:25:00,904] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:25:10,911] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:25:20,918] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:25:30,929] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:25:40,937] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:25:50,950] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:26:00,962] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:26:10,969] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:26:20,981] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:26:30,989] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:26:40,991] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:26:50,995] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:27:01,004] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:27:08,227] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 13:27:08,229] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 13:27:08,230] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 13:27:08,230] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 13:27:08,230] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 13:27:08,231] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 13:27:08,231] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 13:27:08,232] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:27:08,232] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:27:08,233] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 13:27:08,233] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 13:27:08,233] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:27:08,234] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:27:08,235] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 13:27:08,552] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:27:08,553] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 13:27:08,667] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 13:27:08,668] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660377233, file=SF-CPU-562-bin.000076, pos=1603226, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 13:27:08,674] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 13:27:08,675] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 13:27:08,678] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:27:08,679] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:27:08,679] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:27:08,680] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:27:08,683] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 13:27:08,686] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:27:08,686] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:27:08,686] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:27:08,687] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:27:08,690] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 13:27:08,691] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 13:27:08,692] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:27:08,692] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 13:27:08,693] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 13:27:08,693] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:27:08,693] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:27:08,694] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 13:27:08,694] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 13:27:08,694] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 13:27:08,696] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:27:08,703] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:27:08,704] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 13:27:08,704] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 13:27:08,709] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 13:27:08,709] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:27:08,710] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:27:08,710] INFO [connector1|task-0] Kafka startTimeMs: 1660377428709 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:27:08,711] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 13:27:08,712] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,712] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,712] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,712] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,713] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,713] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,713] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,713] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:27:08,714] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,714] INFO [connector1|task-0]    transformation = or conversion errors (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,714] INFO [connector1|task-0]    transforms.unwrap.add.source.fields = table (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,715] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,715] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,715] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,716] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,716] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,717] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,717] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,718] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,718] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,719] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,719] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,719] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,719] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:27:08,728] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1602862} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 13:27:08,739] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 13:27:08,739] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 13:27:08,740] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 13:27:08,740] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 13:27:08,744] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:27:08,744] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:27:08,744] INFO [connector1|task-0] Kafka startTimeMs: 1660377428744 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:27:08,744] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 13:27:08,745] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 13:27:08,746] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:27:08,747] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:27:08,751] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:27:08,751] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:27:08,751] INFO [connector1|task-0] Kafka startTimeMs: 1660377428751 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:27:08,754] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:27:08,756] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:27:08,756] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:27:08,756] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:27:08,757] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:27:08,763] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:27:08,765] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:27:08,766] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:27:08,770] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:27:08,770] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:27:08,771] INFO [connector1|task-0] Kafka startTimeMs: 1660377428770 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:27:08,771] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 13:27:08,772] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 13:27:08,777] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:27:08,780] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:27:08,780] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:27:08,780] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:27:08,777] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:27:08,780] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:27:08,781] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:27:08,781] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:27:08,781] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:27:08,781] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:27:08,781] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:27:08,781] INFO [connector1|task-0] Kafka startTimeMs: 1660377428781 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:27:08,787] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:27:08,787] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:27:08,787] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:27:08,788] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:27:08,788] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:27:08,790] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:27:08,797] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:27:08,800] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 13:27:08,800] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:27:08,804] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:27:08,804] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:27:08,804] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:27:08,804] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:27:08,811] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:27:08,811] INFO [connector1|task-0] Kafka startTimeMs: 1660377428804 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:27:08,815] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:27:08,817] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:27:08,817] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:27:08,817] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:27:08,818] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:27:08,818] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:27:08,819] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:27:08,819] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:27:08,823] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:27:08,826] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:27:08,826] INFO [connector1|task-0] Kafka startTimeMs: 1660377428823 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:27:08,829] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:27:08,830] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:27:08,835] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:27:08,835] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:27:08,835] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:27:08,835] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:27:08,836] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:27:08,840] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:27:08,840] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 13:27:08,841] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:27:08,845] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:27:08,845] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:27:08,845] INFO [connector1|task-0] Kafka startTimeMs: 1660377428845 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:27:08,846] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 13:27:08,851] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:27:08,856] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:27:08,860] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 13:27:08,861] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 13:27:08,864] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:27:08,864] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 13:27:08,867] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=1, memberId='order_server-dbhistory-dd4b1eba-0ea2-4d14-a94d-933630b2cfaf', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 13:27:08,880] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 1: {order_server-dbhistory-dd4b1eba-0ea2-4d14-a94d-933630b2cfaf=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 13:27:08,882] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=1, memberId='order_server-dbhistory-dd4b1eba-0ea2-4d14-a94d-933630b2cfaf', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 13:27:08,882] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 13:27:08,882] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 13:27:08,884] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 13:27:08,885] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 13:27:08,911] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 13:27:08,911] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-dd4b1eba-0ea2-4d14-a94d-933630b2cfaf sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 13:27:08,911] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:27:08,912] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:27:08,916] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:27:08,917] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:27:08,918] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:27:08,919] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:27:08,919] INFO [connector1|task-0] Finished database history recovery of 21 change(s) in 78 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 13:27:08,920] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 13:27:08,923] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 13:27:08,925] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 13:27:08,925] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 13:27:08,925] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 13:27:08,926] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 13:27:08,926] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 13:27:08,926] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 13:27:08,926] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 13:27:08,926] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 13:27:08,927] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1602862, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1602862, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 13:27:08,927] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 13:27:08,927] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 13:27:08,931] INFO [connector1|task-0] Skip 0 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 13:27:08,942] INFO [connector1|task-0] Skip 0 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 13:27:08,942] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:27:08,943] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:27:08,955] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1602862, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1602862, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 13:27:08,956] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 13:27:08,956] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:27:09,057] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 13:27:18,724] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:27:28,731] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:27:38,744] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:27:48,750] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:27:58,771] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:28:08,780] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:28:13,501] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2022-08-13 13:28:13,501] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:311)
[2022-08-13 13:28:13,505] INFO Stopped http_8083@42f22995{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2022-08-13 13:28:13,505] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2022-08-13 13:28:13,506] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:328)
[2022-08-13 13:28:13,506] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:106)
[2022-08-13 13:28:13,507] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 13:28:13,810] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:28:13,811] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 13:28:13,920] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 13:28:13,920] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1603590, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 13:28:13,925] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 13:28:13,926] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 13:28:13,930] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:28:13,931] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:28:13,932] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:28:13,933] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:28:13,933] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 13:28:13,935] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:28:13,937] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:28:13,937] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:28:13,937] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:28:13,938] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 13:28:13,938] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 13:28:13,938] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 13:28:13,938] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:199)
[2022-08-13 13:28:13,939] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2022-08-13 13:28:13,939] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:28:13,939] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:28:13,939] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:28:13,940] INFO App info kafka.connect for 192.168.0.111:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:28:13,940] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:220)
[2022-08-13 13:28:13,940] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:124)
[2022-08-13 13:28:13,940] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2022-08-13 13:28:18,551] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:68)
[2022-08-13 13:28:18,559] INFO WorkerInfo values: 
	jvm.args = -Xmx256M, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=C:\kafka/logs, -Dlog4j.configuration=file:C:\kafka/config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 17.0.2, 17.0.2+8-LTS-86
	jvm.classpath = C:\kafka\libs\activation-1.1.1.jar;C:\kafka\libs\aopalliance-repackaged-2.6.1.jar;C:\kafka\libs\argparse4j-0.7.0.jar;C:\kafka\libs\audience-annotations-0.5.0.jar;C:\kafka\libs\commons-cli-1.4.jar;C:\kafka\libs\commons-lang3-3.8.1.jar;C:\kafka\libs\connect-api-3.1.0.jar;C:\kafka\libs\connect-basic-auth-extension-3.1.0.jar;C:\kafka\libs\connect-file-3.1.0.jar;C:\kafka\libs\connect-json-3.1.0.jar;C:\kafka\libs\connect-mirror-3.1.0.jar;C:\kafka\libs\connect-mirror-client-3.1.0.jar;C:\kafka\libs\connect-runtime-3.1.0.jar;C:\kafka\libs\connect-transforms-3.1.0.jar;C:\kafka\libs\hk2-api-2.6.1.jar;C:\kafka\libs\hk2-locator-2.6.1.jar;C:\kafka\libs\hk2-utils-2.6.1.jar;C:\kafka\libs\jackson-annotations-2.12.3.jar;C:\kafka\libs\jackson-core-2.12.3.jar;C:\kafka\libs\jackson-databind-2.12.3.jar;C:\kafka\libs\jackson-dataformat-csv-2.12.3.jar;C:\kafka\libs\jackson-datatype-jdk8-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-base-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-json-provider-2.12.3.jar;C:\kafka\libs\jackson-module-jaxb-annotations-2.12.3.jar;C:\kafka\libs\jackson-module-scala_2.13-2.12.3.jar;C:\kafka\libs\jakarta.activation-api-1.2.1.jar;C:\kafka\libs\jakarta.annotation-api-1.3.5.jar;C:\kafka\libs\jakarta.inject-2.6.1.jar;C:\kafka\libs\jakarta.validation-api-2.0.2.jar;C:\kafka\libs\jakarta.ws.rs-api-2.1.6.jar;C:\kafka\libs\jakarta.xml.bind-api-2.3.2.jar;C:\kafka\libs\javassist-3.27.0-GA.jar;C:\kafka\libs\javax.servlet-api-3.1.0.jar;C:\kafka\libs\javax.ws.rs-api-2.1.1.jar;C:\kafka\libs\jaxb-api-2.3.0.jar;C:\kafka\libs\jersey-client-2.34.jar;C:\kafka\libs\jersey-common-2.34.jar;C:\kafka\libs\jersey-container-servlet-2.34.jar;C:\kafka\libs\jersey-container-servlet-core-2.34.jar;C:\kafka\libs\jersey-hk2-2.34.jar;C:\kafka\libs\jersey-server-2.34.jar;C:\kafka\libs\jetty-client-9.4.43.v20210629.jar;C:\kafka\libs\jetty-continuation-9.4.43.v20210629.jar;C:\kafka\libs\jetty-http-9.4.43.v20210629.jar;C:\kafka\libs\jetty-io-9.4.43.v20210629.jar;C:\kafka\libs\jetty-security-9.4.43.v20210629.jar;C:\kafka\libs\jetty-server-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlet-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlets-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-ajax-9.4.43.v20210629.jar;C:\kafka\libs\jline-3.12.1.jar;C:\kafka\libs\jopt-simple-5.0.4.jar;C:\kafka\libs\jose4j-0.7.8.jar;C:\kafka\libs\kafka-clients-3.1.0.jar;C:\kafka\libs\kafka-log4j-appender-3.1.0.jar;C:\kafka\libs\kafka-metadata-3.1.0.jar;C:\kafka\libs\kafka-raft-3.1.0.jar;C:\kafka\libs\kafka-server-common-3.1.0.jar;C:\kafka\libs\kafka-shell-3.1.0.jar;C:\kafka\libs\kafka-storage-3.1.0.jar;C:\kafka\libs\kafka-storage-api-3.1.0.jar;C:\kafka\libs\kafka-streams-3.1.0.jar;C:\kafka\libs\kafka-streams-examples-3.1.0.jar;C:\kafka\libs\kafka-streams-scala_2.13-3.1.0.jar;C:\kafka\libs\kafka-streams-test-utils-3.1.0.jar;C:\kafka\libs\kafka-tools-3.1.0.jar;C:\kafka\libs\kafka_2.13-3.1.0.jar;C:\kafka\libs\log4j-1.2.17.jar;C:\kafka\libs\lz4-java-1.8.0.jar;C:\kafka\libs\maven-artifact-3.8.1.jar;C:\kafka\libs\metrics-core-2.2.0.jar;C:\kafka\libs\metrics-core-4.1.12.1.jar;C:\kafka\libs\netty-buffer-4.1.68.Final.jar;C:\kafka\libs\netty-codec-4.1.68.Final.jar;C:\kafka\libs\netty-common-4.1.68.Final.jar;C:\kafka\libs\netty-handler-4.1.68.Final.jar;C:\kafka\libs\netty-resolver-4.1.68.Final.jar;C:\kafka\libs\netty-transport-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-epoll-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-unix-common-4.1.68.Final.jar;C:\kafka\libs\osgi-resource-locator-1.0.3.jar;C:\kafka\libs\paranamer-2.8.jar;C:\kafka\libs\plexus-utils-3.2.1.jar;C:\kafka\libs\reflections-0.9.12.jar;C:\kafka\libs\rocksdbjni-6.22.1.1.jar;C:\kafka\libs\scala-collection-compat_2.13-2.4.4.jar;C:\kafka\libs\scala-java8-compat_2.13-1.0.0.jar;C:\kafka\libs\scala-library-2.13.6.jar;C:\kafka\libs\scala-logging_2.13-3.9.3.jar;C:\kafka\libs\scala-reflect-2.13.6.jar;C:\kafka\libs\slf4j-api-1.7.30.jar;C:\kafka\libs\slf4j-log4j12-1.7.30.jar;C:\kafka\libs\snappy-java-1.1.8.4.jar;C:\kafka\libs\trogdor-3.1.0.jar;C:\kafka\libs\zookeeper-3.6.3.jar;C:\kafka\libs\zookeeper-jute-3.6.3.jar;C:\kafka\libs\zstd-jni-1.5.0-4.jar
	os.spec = Windows 10, amd64, 10.0
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2022-08-13 13:28:18,561] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:77)
[2022-08-13 13:28:18,570] INFO Loading plugin from: C:\kafka\opt\connectors\debezium-debezium-connector-mysql-1.9.3 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:265)
[2022-08-13 13:28:18,943] INFO Registered loader: PluginClassLoader{pluginLocation=file:/C:/kafka/opt/connectors/debezium-debezium-connector-mysql-1.9.3/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 13:28:18,943] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,945] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,945] INFO Added plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,945] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,945] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,945] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,945] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,946] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,946] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,946] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,946] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,947] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:18,948] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,709] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@266474c2 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 13:28:19,710] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,711] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,711] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,711] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,711] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,711] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,711] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,712] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,712] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,712] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,712] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,712] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,712] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,712] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,713] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,713] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,713] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,713] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,713] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,713] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,713] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,713] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,714] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,714] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,714] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,714] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,714] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,714] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,714] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,715] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,715] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,715] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,715] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,715] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,715] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,715] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,716] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,716] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,716] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,716] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,716] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,716] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,716] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,717] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,717] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,717] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,717] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,717] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,717] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,717] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,717] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,717] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,718] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:28:19,718] INFO Added aliases 'MySqlConnector' and 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,718] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,718] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,719] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,719] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,719] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,719] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,719] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,719] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,719] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,720] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,720] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,720] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,720] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,720] INFO Added aliases 'CloudEventsConverter' and 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,720] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,720] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,721] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,721] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,721] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,721] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,721] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,721] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,721] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,721] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,721] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,722] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,722] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,722] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,722] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,722] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,722] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,722] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,722] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,723] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,723] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,723] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,723] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,723] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,723] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,723] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,723] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,724] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,724] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,724] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,724] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,724] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,724] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:28:19,724] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,724] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,725] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:28:19,739] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = All
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = C:/kafka/tmp/connect.offsets
	plugin.path = [/opt/connectors, C:/kafka/opt/connectors]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:376)
[2022-08-13 13:28:19,740] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 13:28:19,743] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 13:28:19,813] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:19,814] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:19,815] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:19,815] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:19,815] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:19,815] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:19,815] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:19,815] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:28:19,816] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:28:19,816] INFO Kafka startTimeMs: 1660377499815 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:28:20,048] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 13:28:20,049] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:28:20,054] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:28:20,054] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:28:20,054] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:28:20,062] INFO Logging initialized @1856ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2022-08-13 13:28:20,089] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:117)
[2022-08-13 13:28:20,089] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:188)
[2022-08-13 13:28:20,094] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 17.0.2+8-LTS-86 (org.eclipse.jetty.server.Server:375)
[2022-08-13 13:28:20,112] INFO Started http_8083@66434cc8{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2022-08-13 13:28:20,112] INFO Started @1906ms (org.eclipse.jetty.server.Server:415)
[2022-08-13 13:28:20,128] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 13:28:20,128] INFO REST server listening at http://192.168.0.111:8083/, advertising URL http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2022-08-13 13:28:20,129] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 13:28:20,129] INFO REST admin endpoints at http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2022-08-13 13:28:20,129] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 13:28:20,129] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:44)
[2022-08-13 13:28:20,134] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 13:28:20,134] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 13:28:20,140] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:20,140] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:20,141] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:20,147] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:20,147] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:20,148] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:20,148] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:20,148] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:28:20,148] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:28:20,148] INFO Kafka startTimeMs: 1660377500148 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:28:20,159] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 13:28:20,160] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:28:20,164] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:28:20,164] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:28:20,164] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:28:20,167] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:28:20,168] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:28:20,168] INFO Kafka startTimeMs: 1660377500167 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:28:20,257] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:28:20,260] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:28:20,273] INFO Kafka Connect standalone worker initialization took 1719ms (org.apache.kafka.connect.cli.ConnectStandalone:99)
[2022-08-13 13:28:20,273] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2022-08-13 13:28:20,274] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:98)
[2022-08-13 13:28:20,274] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:185)
[2022-08-13 13:28:20,274] INFO Starting FileOffsetBackingStore with file C:\kafka\tmp\connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2022-08-13 13:28:20,282] INFO Worker started (org.apache.kafka.connect.runtime.Worker:192)
[2022-08-13 13:28:20,282] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:101)
[2022-08-13 13:28:20,283] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:208)
[2022-08-13 13:28:20,309] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:225)
[2022-08-13 13:28:20,347] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2022-08-13 13:28:20,347] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2022-08-13 13:28:20,349] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2022-08-13 13:28:20,699] INFO Started o.e.j.s.ServletContextHandler@221dad51{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:915)
[2022-08-13 13:28:20,699] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2022-08-13 13:28:20,700] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2022-08-13 13:28:20,875] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 13:28:20,881] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 13:28:20,882] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 13:28:20,890] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 13:28:20,890] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:28:20,891] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:28:20,895] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 13:28:20,898] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 13:28:20,900] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:28:20,901] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:28:20,903] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 13:28:20,905] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 13:28:20,906] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:28:20,908] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 13:28:20,908] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 13:28:20,909] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:28:20,909] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:28:20,909] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 13:28:20,909] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 13:28:20,910] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 13:28:20,917] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:28:20,918] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:28:20,922] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 13:28:20,928] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 13:28:20,950] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 13:28:20,950] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:28:20,950] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:28:20,950] INFO [connector1|task-0] Kafka startTimeMs: 1660377500950 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:28:20,959] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:28:20,961] INFO Created connector connector1 (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2022-08-13 13:28:20,962] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 13:28:20,963] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,963] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,963] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,964] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,964] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,964] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,964] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,964] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,964] INFO [connector1|task-0]    transforms.unwrap.add.source.fields = table (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,964] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,964] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,964] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,965] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,965] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,965] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,965] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,965] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,965] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,965] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,965] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,966] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:20,966] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:28:21,019] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1602862} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 13:28:21,046] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 13:28:21,046] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 13:28:21,047] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 13:28:21,048] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 13:28:21,067] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:28:21,069] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:28:21,069] INFO [connector1|task-0] Kafka startTimeMs: 1660377501067 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:28:21,074] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:28:21,089] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 13:28:21,090] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 13:28:21,095] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:28:21,120] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:28:21,120] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:28:21,120] INFO [connector1|task-0] Kafka startTimeMs: 1660377501120 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:28:21,126] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:28:21,129] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:28:21,129] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:28:21,130] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:28:21,130] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:28:21,130] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:28:21,131] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:28:21,131] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:28:21,135] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:28:21,135] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:28:21,135] INFO [connector1|task-0] Kafka startTimeMs: 1660377501135 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:28:21,136] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 13:28:21,137] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 13:28:21,143] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:21,148] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:28:21,149] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:21,150] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:21,150] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:21,150] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:21,150] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:28:21,150] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:21,151] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:28:21,151] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:28:21,151] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:28:21,151] INFO [connector1|task-0] Kafka startTimeMs: 1660377501151 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:28:21,164] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:28:21,165] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:28:21,165] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:28:21,166] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:28:21,166] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:28:21,168] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:28:21,169] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:28:21,174] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:28:21,174] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:28:21,175] INFO [connector1|task-0] Kafka startTimeMs: 1660377501174 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:28:21,177] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 13:28:21,181] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:28:21,182] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:28:21,182] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:28:21,182] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:28:21,183] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:28:21,184] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:28:21,185] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:28:21,185] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:28:21,185] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:28:21,185] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:28:21,186] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:28:21,186] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:28:21,190] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:28:21,190] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:28:21,191] INFO [connector1|task-0] Kafka startTimeMs: 1660377501190 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:28:21,200] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:28:21,200] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:28:21,204] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:28:21,205] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:28:21,205] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:28:21,205] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:28:21,206] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:28:21,207] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:28:21,209] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 13:28:21,214] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:28:21,218] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:28:21,218] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:28:21,218] INFO [connector1|task-0] Kafka startTimeMs: 1660377501218 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:28:21,219] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 13:28:21,223] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:28:21,228] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:28:21,232] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 13:28:21,233] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 13:28:21,242] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:28:21,243] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 13:28:21,245] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=3, memberId='order_server-dbhistory-4590b203-b39a-4d4f-9a96-fde7fcc0ebeb', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 13:28:21,247] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 3: {order_server-dbhistory-4590b203-b39a-4d4f-9a96-fde7fcc0ebeb=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 13:28:21,251] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=3, memberId='order_server-dbhistory-4590b203-b39a-4d4f-9a96-fde7fcc0ebeb', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 13:28:21,251] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 13:28:21,253] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 13:28:21,261] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 13:28:21,265] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 13:28:21,706] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 13:28:21,706] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-4590b203-b39a-4d4f-9a96-fde7fcc0ebeb sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 13:28:21,708] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:28:21,708] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:28:21,712] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:28:21,712] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:28:21,712] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:28:21,713] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:28:21,714] INFO [connector1|task-0] Finished database history recovery of 21 change(s) in 505 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 13:28:21,724] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 13:28:21,729] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 13:28:21,735] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 13:28:21,755] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 13:28:21,757] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 13:28:21,758] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 13:28:21,758] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 13:28:21,762] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 13:28:21,763] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 13:28:21,767] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 13:28:21,768] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1602862, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1602862, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 13:28:21,771] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 13:28:21,772] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 13:28:21,781] INFO [connector1|task-0] Skip 0 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 13:28:21,781] INFO [connector1|task-0] Skip 0 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 13:28:21,782] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:28:21,788] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:28:21,798] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1602862, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1602862, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 13:28:21,799] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 13:28:21,799] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:28:21,905] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 13:28:30,972] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:28:40,985] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:28:50,991] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:29:01,009] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:29:11,026] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:29:21,031] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:29:31,047] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:29:41,063] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:29:51,066] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:30:01,080] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:30:11,094] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:30:21,097] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:30:31,098] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:30:41,100] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:30:51,110] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:31:01,120] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:31:11,132] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:31:21,134] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:31:31,142] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:31:41,155] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:31:51,164] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:32:01,178] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:32:11,192] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:32:21,206] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:32:31,208] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:32:41,223] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:32:51,250] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:33:01,264] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:33:11,272] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:33:21,288] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:33:31,291] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:33:41,293] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:33:51,298] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:34:01,310] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:34:11,313] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:34:21,327] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:34:31,339] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:34:41,350] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:34:51,357] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:35:01,360] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:35:11,368] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:35:21,372] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:35:22,387] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2022-08-13 13:35:22,387] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:311)
[2022-08-13 13:35:22,391] INFO Stopped http_8083@66434cc8{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2022-08-13 13:35:22,392] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2022-08-13 13:35:22,393] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:328)
[2022-08-13 13:35:22,393] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:106)
[2022-08-13 13:35:22,394] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 13:35:22,679] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:35:22,679] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 13:35:22,695] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 13:35:22,696] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1604682, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 13:35:22,698] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 13:35:22,701] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 13:35:22,703] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:35:22,704] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:35:22,704] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:35:22,704] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:35:22,705] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 13:35:22,707] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:35:22,708] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:35:22,708] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:35:22,709] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:35:22,712] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 13:35:22,715] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 13:35:22,715] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 13:35:22,716] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:199)
[2022-08-13 13:35:22,717] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2022-08-13 13:35:22,717] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:35:22,717] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:35:22,718] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:35:22,718] INFO App info kafka.connect for 192.168.0.111:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:35:22,718] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:220)
[2022-08-13 13:35:22,719] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:124)
[2022-08-13 13:35:22,720] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2022-08-13 13:36:23,373] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:68)
[2022-08-13 13:36:23,382] INFO WorkerInfo values: 
	jvm.args = -Xmx256M, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=C:\kafka/logs, -Dlog4j.configuration=file:C:\kafka/config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 17.0.2, 17.0.2+8-LTS-86
	jvm.classpath = C:\kafka\libs\activation-1.1.1.jar;C:\kafka\libs\aopalliance-repackaged-2.6.1.jar;C:\kafka\libs\argparse4j-0.7.0.jar;C:\kafka\libs\audience-annotations-0.5.0.jar;C:\kafka\libs\commons-cli-1.4.jar;C:\kafka\libs\commons-lang3-3.8.1.jar;C:\kafka\libs\connect-api-3.1.0.jar;C:\kafka\libs\connect-basic-auth-extension-3.1.0.jar;C:\kafka\libs\connect-file-3.1.0.jar;C:\kafka\libs\connect-json-3.1.0.jar;C:\kafka\libs\connect-mirror-3.1.0.jar;C:\kafka\libs\connect-mirror-client-3.1.0.jar;C:\kafka\libs\connect-runtime-3.1.0.jar;C:\kafka\libs\connect-transforms-3.1.0.jar;C:\kafka\libs\hk2-api-2.6.1.jar;C:\kafka\libs\hk2-locator-2.6.1.jar;C:\kafka\libs\hk2-utils-2.6.1.jar;C:\kafka\libs\jackson-annotations-2.12.3.jar;C:\kafka\libs\jackson-core-2.12.3.jar;C:\kafka\libs\jackson-databind-2.12.3.jar;C:\kafka\libs\jackson-dataformat-csv-2.12.3.jar;C:\kafka\libs\jackson-datatype-jdk8-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-base-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-json-provider-2.12.3.jar;C:\kafka\libs\jackson-module-jaxb-annotations-2.12.3.jar;C:\kafka\libs\jackson-module-scala_2.13-2.12.3.jar;C:\kafka\libs\jakarta.activation-api-1.2.1.jar;C:\kafka\libs\jakarta.annotation-api-1.3.5.jar;C:\kafka\libs\jakarta.inject-2.6.1.jar;C:\kafka\libs\jakarta.validation-api-2.0.2.jar;C:\kafka\libs\jakarta.ws.rs-api-2.1.6.jar;C:\kafka\libs\jakarta.xml.bind-api-2.3.2.jar;C:\kafka\libs\javassist-3.27.0-GA.jar;C:\kafka\libs\javax.servlet-api-3.1.0.jar;C:\kafka\libs\javax.ws.rs-api-2.1.1.jar;C:\kafka\libs\jaxb-api-2.3.0.jar;C:\kafka\libs\jersey-client-2.34.jar;C:\kafka\libs\jersey-common-2.34.jar;C:\kafka\libs\jersey-container-servlet-2.34.jar;C:\kafka\libs\jersey-container-servlet-core-2.34.jar;C:\kafka\libs\jersey-hk2-2.34.jar;C:\kafka\libs\jersey-server-2.34.jar;C:\kafka\libs\jetty-client-9.4.43.v20210629.jar;C:\kafka\libs\jetty-continuation-9.4.43.v20210629.jar;C:\kafka\libs\jetty-http-9.4.43.v20210629.jar;C:\kafka\libs\jetty-io-9.4.43.v20210629.jar;C:\kafka\libs\jetty-security-9.4.43.v20210629.jar;C:\kafka\libs\jetty-server-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlet-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlets-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-ajax-9.4.43.v20210629.jar;C:\kafka\libs\jline-3.12.1.jar;C:\kafka\libs\jopt-simple-5.0.4.jar;C:\kafka\libs\jose4j-0.7.8.jar;C:\kafka\libs\kafka-clients-3.1.0.jar;C:\kafka\libs\kafka-log4j-appender-3.1.0.jar;C:\kafka\libs\kafka-metadata-3.1.0.jar;C:\kafka\libs\kafka-raft-3.1.0.jar;C:\kafka\libs\kafka-server-common-3.1.0.jar;C:\kafka\libs\kafka-shell-3.1.0.jar;C:\kafka\libs\kafka-storage-3.1.0.jar;C:\kafka\libs\kafka-storage-api-3.1.0.jar;C:\kafka\libs\kafka-streams-3.1.0.jar;C:\kafka\libs\kafka-streams-examples-3.1.0.jar;C:\kafka\libs\kafka-streams-scala_2.13-3.1.0.jar;C:\kafka\libs\kafka-streams-test-utils-3.1.0.jar;C:\kafka\libs\kafka-tools-3.1.0.jar;C:\kafka\libs\kafka_2.13-3.1.0.jar;C:\kafka\libs\log4j-1.2.17.jar;C:\kafka\libs\lz4-java-1.8.0.jar;C:\kafka\libs\maven-artifact-3.8.1.jar;C:\kafka\libs\metrics-core-2.2.0.jar;C:\kafka\libs\metrics-core-4.1.12.1.jar;C:\kafka\libs\netty-buffer-4.1.68.Final.jar;C:\kafka\libs\netty-codec-4.1.68.Final.jar;C:\kafka\libs\netty-common-4.1.68.Final.jar;C:\kafka\libs\netty-handler-4.1.68.Final.jar;C:\kafka\libs\netty-resolver-4.1.68.Final.jar;C:\kafka\libs\netty-transport-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-epoll-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-unix-common-4.1.68.Final.jar;C:\kafka\libs\osgi-resource-locator-1.0.3.jar;C:\kafka\libs\paranamer-2.8.jar;C:\kafka\libs\plexus-utils-3.2.1.jar;C:\kafka\libs\reflections-0.9.12.jar;C:\kafka\libs\rocksdbjni-6.22.1.1.jar;C:\kafka\libs\scala-collection-compat_2.13-2.4.4.jar;C:\kafka\libs\scala-java8-compat_2.13-1.0.0.jar;C:\kafka\libs\scala-library-2.13.6.jar;C:\kafka\libs\scala-logging_2.13-3.9.3.jar;C:\kafka\libs\scala-reflect-2.13.6.jar;C:\kafka\libs\slf4j-api-1.7.30.jar;C:\kafka\libs\slf4j-log4j12-1.7.30.jar;C:\kafka\libs\snappy-java-1.1.8.4.jar;C:\kafka\libs\trogdor-3.1.0.jar;C:\kafka\libs\zookeeper-3.6.3.jar;C:\kafka\libs\zookeeper-jute-3.6.3.jar;C:\kafka\libs\zstd-jni-1.5.0-4.jar
	os.spec = Windows 10, amd64, 10.0
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2022-08-13 13:36:23,384] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:77)
[2022-08-13 13:36:23,396] INFO Loading plugin from: C:\kafka\opt\connectors\debezium-debezium-connector-mysql-1.9.3 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:265)
[2022-08-13 13:36:23,762] INFO Registered loader: PluginClassLoader{pluginLocation=file:/C:/kafka/opt/connectors/debezium-debezium-connector-mysql-1.9.3/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 13:36:23,763] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,764] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,764] INFO Added plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,764] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,765] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,765] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,765] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,765] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,765] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,765] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,766] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,767] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:23,767] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,511] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@266474c2 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 13:36:24,511] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,512] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,513] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,513] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,513] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,513] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,513] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,513] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,513] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,514] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,514] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,514] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,514] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,514] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,514] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,514] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,514] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,515] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,515] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,515] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,515] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,515] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,515] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,515] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,516] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,516] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,516] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,516] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,516] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,516] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,516] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,516] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,517] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,517] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,517] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,517] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,517] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,517] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,517] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,518] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,518] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,518] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,518] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,518] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,518] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,518] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,519] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,519] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,519] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,519] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,519] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,519] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,519] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 13:36:24,520] INFO Added aliases 'MySqlConnector' and 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,520] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,520] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,520] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,520] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,521] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,521] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,521] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,521] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,521] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,521] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,521] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,521] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,522] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,522] INFO Added aliases 'CloudEventsConverter' and 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,522] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,522] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,522] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,522] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,522] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,522] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,522] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,529] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,529] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,529] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,529] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,529] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,529] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,529] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,530] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,530] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,530] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,530] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,530] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,530] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,530] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,530] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,531] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,531] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,531] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,531] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,531] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,531] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,531] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,531] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,532] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,532] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,532] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 13:36:24,532] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,532] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,532] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 13:36:24,545] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = All
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = C:/kafka/tmp/connect.offsets
	plugin.path = [/opt/connectors, C:/kafka/opt/connectors]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:376)
[2022-08-13 13:36:24,546] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 13:36:24,549] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 13:36:24,621] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,621] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,622] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,622] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,622] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,623] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,623] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,623] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:36:24,623] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:36:24,623] INFO Kafka startTimeMs: 1660377984623 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:36:24,849] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 13:36:24,850] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:36:24,856] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:36:24,856] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:36:24,856] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:36:24,863] INFO Logging initialized @1840ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2022-08-13 13:36:24,889] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:117)
[2022-08-13 13:36:24,889] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:188)
[2022-08-13 13:36:24,894] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 17.0.2+8-LTS-86 (org.eclipse.jetty.server.Server:375)
[2022-08-13 13:36:24,913] INFO Started http_8083@66434cc8{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2022-08-13 13:36:24,913] INFO Started @1890ms (org.eclipse.jetty.server.Server:415)
[2022-08-13 13:36:24,928] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 13:36:24,928] INFO REST server listening at http://192.168.0.111:8083/, advertising URL http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2022-08-13 13:36:24,928] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 13:36:24,929] INFO REST admin endpoints at http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2022-08-13 13:36:24,929] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 13:36:24,929] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:44)
[2022-08-13 13:36:24,935] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 13:36:24,935] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 13:36:24,940] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,942] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,942] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,942] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,942] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,942] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,942] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:24,942] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:36:24,943] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:36:24,943] INFO Kafka startTimeMs: 1660377984942 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:36:24,953] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 13:36:24,955] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:36:24,957] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:36:24,957] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:36:24,958] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:36:24,961] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:36:24,961] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:36:24,961] INFO Kafka startTimeMs: 1660377984961 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:36:25,044] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:36:25,046] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:36:25,056] INFO Kafka Connect standalone worker initialization took 1680ms (org.apache.kafka.connect.cli.ConnectStandalone:99)
[2022-08-13 13:36:25,056] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2022-08-13 13:36:25,057] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:98)
[2022-08-13 13:36:25,057] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:185)
[2022-08-13 13:36:25,058] INFO Starting FileOffsetBackingStore with file C:\kafka\tmp\connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2022-08-13 13:36:25,063] INFO Worker started (org.apache.kafka.connect.runtime.Worker:192)
[2022-08-13 13:36:25,063] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:101)
[2022-08-13 13:36:25,063] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:208)
[2022-08-13 13:36:25,088] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:225)
[2022-08-13 13:36:25,133] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2022-08-13 13:36:25,134] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2022-08-13 13:36:25,135] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2022-08-13 13:36:25,463] INFO Started o.e.j.s.ServletContextHandler@221dad51{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:915)
[2022-08-13 13:36:25,463] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2022-08-13 13:36:25,463] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2022-08-13 13:36:25,646] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 13:36:25,651] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 13:36:25,654] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 13:36:25,661] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 13:36:25,661] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:36:25,662] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:36:25,666] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 13:36:25,667] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 13:36:25,670] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:36:25,672] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:36:25,674] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 13:36:25,676] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 13:36:25,677] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:36:25,678] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 13:36:25,679] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 13:36:25,687] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:36:25,687] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 13:36:25,687] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 13:36:25,688] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 13:36:25,688] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 13:36:25,690] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 13:36:25,691] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 13:36:25,694] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 13:36:25,699] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 13:36:25,720] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 13:36:25,721] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:36:25,721] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:36:25,722] INFO [connector1|task-0] Kafka startTimeMs: 1660377985721 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:36:25,729] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:36:25,730] INFO Created connector connector1 (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2022-08-13 13:36:25,730] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 13:36:25,732] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,732] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,732] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,732] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,732] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,732] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,733] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,733] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,733] INFO [connector1|task-0]    transforms.unwrap.add.source.fields = table (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,733] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,733] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,733] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,733] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,734] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,734] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,734] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,735] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,735] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,735] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,735] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,735] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,735] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 13:36:25,780] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1602862} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 13:36:25,806] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 13:36:25,807] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 13:36:25,807] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 13:36:25,809] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 13:36:25,813] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:36:25,813] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:36:25,814] INFO [connector1|task-0] Kafka startTimeMs: 1660377985813 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:36:25,817] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:36:25,831] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 13:36:25,832] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 13:36:25,839] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:36:25,864] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:36:25,864] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:36:25,864] INFO [connector1|task-0] Kafka startTimeMs: 1660377985864 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:36:25,870] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:36:25,875] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:36:25,875] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:36:25,875] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:36:25,876] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:36:25,876] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:36:25,877] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:36:25,877] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:36:25,881] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:36:25,881] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:36:25,881] INFO [connector1|task-0] Kafka startTimeMs: 1660377985881 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:36:25,882] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 13:36:25,883] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 13:36:25,893] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:25,894] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:25,894] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:25,894] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:36:25,894] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:25,894] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:25,895] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:25,895] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 13:36:25,895] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:36:25,895] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:36:25,895] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:36:25,895] INFO [connector1|task-0] Kafka startTimeMs: 1660377985895 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:36:25,916] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:36:25,920] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:36:25,921] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:36:25,921] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:36:25,921] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:36:25,922] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:36:25,923] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:36:25,927] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:36:25,928] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:36:25,928] INFO [connector1|task-0] Kafka startTimeMs: 1660377985927 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:36:25,933] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:36:25,939] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 13:36:25,939] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:36:25,940] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:36:25,940] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:36:25,941] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:36:25,941] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:36:25,941] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:36:25,941] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:36:25,941] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:36:25,941] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:36:25,942] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:36:25,943] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:36:25,947] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:36:25,947] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:36:25,947] INFO [connector1|task-0] Kafka startTimeMs: 1660377985947 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:36:25,951] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:36:25,952] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:36:25,957] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:36:25,958] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:36:25,958] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:36:25,958] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:36:25,958] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:36:25,959] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:36:25,959] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 13:36:25,965] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 13:36:25,971] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 13:36:25,971] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 13:36:25,972] INFO [connector1|task-0] Kafka startTimeMs: 1660377985971 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 13:36:25,973] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 13:36:25,977] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:36:25,982] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 13:36:25,988] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 13:36:25,990] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 13:36:26,004] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:36:26,004] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 13:36:26,013] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=1, memberId='order_server-dbhistory-dcbca7a1-0dd6-49f6-9c82-8e8d006f1451', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 13:36:26,015] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 1: {order_server-dbhistory-dcbca7a1-0dd6-49f6-9c82-8e8d006f1451=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 13:36:26,049] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=1, memberId='order_server-dbhistory-dcbca7a1-0dd6-49f6-9c82-8e8d006f1451', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 13:36:26,050] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 13:36:26,052] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 13:36:26,065] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 13:36:26,071] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 13:36:26,530] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 13:36:26,530] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-dcbca7a1-0dd6-49f6-9c82-8e8d006f1451 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 13:36:26,532] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 13:36:26,532] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 13:36:26,545] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 13:36:26,545] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 13:36:26,545] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 13:36:26,547] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 13:36:26,547] INFO [connector1|task-0] Finished database history recovery of 21 change(s) in 588 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 13:36:26,557] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 13:36:26,565] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 13:36:26,568] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 13:36:26,595] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 13:36:26,597] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 13:36:26,598] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 13:36:26,598] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 13:36:26,600] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 13:36:26,601] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 13:36:26,607] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 13:36:26,608] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1602862, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1602862, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 13:36:26,612] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 13:36:26,615] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 13:36:26,625] INFO [connector1|task-0] Skip 0 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 13:36:26,626] INFO [connector1|task-0] Skip 0 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 13:36:26,626] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:36:26,632] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:36:26,641] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1602862, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1602862, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 13:36:26,641] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 13:36:26,642] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 13:36:26,743] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 13:36:35,731] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:36:45,739] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:36:55,744] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:37:05,745] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:37:15,747] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:37:25,759] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:37:35,770] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:37:45,785] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:37:55,790] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:38:05,804] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:38:15,814] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:38:25,817] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:38:35,821] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:38:45,828] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:38:55,834] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:39:05,843] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:39:15,853] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:39:25,862] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:39:35,866] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:39:45,872] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:39:55,873] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:40:05,882] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:40:15,892] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:40:25,897] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:40:35,898] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:40:45,910] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:40:55,917] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:41:05,932] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:41:15,940] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:41:25,955] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:41:35,960] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:41:45,977] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:41:55,980] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:42:05,990] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:42:16,001] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:42:26,007] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:42:36,021] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:42:46,028] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:42:56,039] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:43:06,041] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:43:16,050] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:43:26,053] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:43:36,063] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:43:38,130] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:43:38,153] INFO [connector1|task-0] Already applied 22 database changes (io.debezium.relational.history.DatabaseHistoryMetrics:132)
[2022-08-13 13:43:38,314] INFO [connector1|task-0] 3 records sent during previous 00:07:12.634, last recorded offset: {transaction_id=null, ts_sec=1660378418, file=SF-CPU-562-bin.000076, pos=1606033, server_id=1} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 13:43:38,326] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server-0 to 0 since the associated topicId changed from null to LVkchKJTThukBrB7dOsgOA (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:43:46,076] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:43:48,623] INFO [connector1|task-0] Already applied 25 database changes (io.debezium.relational.history.DatabaseHistoryMetrics:132)
[2022-08-13 13:43:56,085] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:44:06,094] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:44:07,341] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:44:16,096] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:44:26,115] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:44:36,120] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:44:46,136] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:44:56,141] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:45:06,154] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:45:16,164] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:45:26,170] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:45:36,187] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:45:37,372] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 13:45:37,375] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server-0 to 0 since the associated topicId changed from null to LVkchKJTThukBrB7dOsgOA (org.apache.kafka.clients.Metadata:402)
[2022-08-13 13:45:46,201] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:45:49,345] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 13:45:56,209] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:46:06,221] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:46:16,235] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:46:26,239] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:46:36,252] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:46:46,253] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:46:56,261] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:47:06,272] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:47:16,285] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:47:26,286] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:47:36,287] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:47:46,295] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:47:56,301] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:48:06,306] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:48:16,308] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:48:26,310] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:48:36,316] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:48:46,319] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:48:55,881] INFO [connector1|task-0] 3 records sent during previous 00:05:17.567, last recorded offset: {transaction_id=null, ts_sec=1660378735, file=SF-CPU-562-bin.000076, pos=1608968, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 13:48:56,331] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:49:06,333] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:49:16,339] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:49:26,348] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:49:36,359] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:49:46,360] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:49:56,375] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:50:06,377] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:50:16,391] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:50:26,397] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:50:36,414] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:50:46,429] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:50:56,432] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:51:06,434] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:51:16,448] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:51:26,464] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:51:36,473] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:51:46,476] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:51:56,489] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:52:06,506] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:52:16,517] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:52:26,518] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:52:36,527] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:52:46,540] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:52:56,545] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:53:06,550] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:53:16,556] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:53:26,563] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:53:36,576] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:53:46,588] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:53:56,604] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:54:06,610] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:54:16,619] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:54:26,632] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:54:36,646] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:54:46,660] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:54:56,668] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:55:06,681] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:55:16,691] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:55:26,706] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:55:36,723] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:55:46,740] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:55:56,744] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:56:06,755] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:56:16,765] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:56:26,774] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:56:36,790] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:56:46,791] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:56:56,805] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:57:06,817] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:57:16,823] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:57:26,830] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:57:36,834] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:57:46,839] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:57:56,843] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:58:06,851] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:58:16,854] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:58:26,864] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:58:36,878] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:58:46,883] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:58:56,898] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:59:06,910] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:59:16,912] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:59:26,915] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:59:36,920] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:59:46,926] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 13:59:56,930] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
