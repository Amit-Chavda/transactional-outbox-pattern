[2022-08-13 12:00:03,308] ERROR Uncaught exception in REST call to /connector1 (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
javax.ws.rs.NotFoundException: HTTP 404 Not Found
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:252)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2022-08-13 12:00:09,616] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:00:19,622] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:00:29,630] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:00:39,644] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:00:49,651] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:00:59,652] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:01:09,658] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:01:19,672] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:01:29,677] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:01:39,685] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:01:49,696] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:01:59,705] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:02:09,707] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:02:19,714] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:02:29,724] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:02:39,739] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:02:49,744] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:02:59,760] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:03:09,774] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:03:19,789] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:03:29,793] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:03:39,797] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:03:49,800] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:03:59,804] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:04:09,813] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:04:19,816] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:04:29,818] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:04:39,835] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:04:49,839] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:04:59,854] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:05:09,861] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:05:19,876] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:05:29,880] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:05:39,889] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:05:49,904] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:05:59,920] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:06:09,924] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:06:19,932] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:06:29,569] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:06:29,677] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:06:29,943] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:06:39,954] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:06:49,960] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:06:59,974] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:07:09,982] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:07:19,996] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:07:29,999] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:07:40,006] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:07:50,013] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:08:00,029] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:08:10,032] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:08:20,048] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:08:30,054] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:08:40,058] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:08:50,073] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:09:00,087] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:09:10,102] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:09:20,108] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:09:30,120] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:09:40,133] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:09:50,141] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:10:00,152] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:10:10,159] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:10:20,173] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:10:30,181] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:10:40,191] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:10:50,197] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:11:00,213] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:11:10,228] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:11:20,238] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:11:29,187] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 12:11:29,189] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:11:29,189] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 12:11:30,243] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:11:40,245] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:11:50,255] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:12:00,262] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:12:10,277] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:12:20,286] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:12:30,556] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:12:40,571] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:12:50,581] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:12:57,219] ERROR Uncaught exception in REST call to /connector-plugins/connector1/config/validate (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
javax.ws.rs.NotAllowedException: HTTP 405 Method Not Allowed
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter.getMethodRouter(MethodSelectingRouter.java:409)
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter.access$000(MethodSelectingRouter.java:73)
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter$4.apply(MethodSelectingRouter.java:674)
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter.apply(MethodSelectingRouter.java:305)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:386)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2022-08-13 12:13:00,590] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:13:10,595] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:13:14,656] ERROR Uncaught exception in REST call to /connector-plugins/connector1/config/validate (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
javax.ws.rs.NotAllowedException: HTTP 405 Method Not Allowed
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter.getMethodRouter(MethodSelectingRouter.java:409)
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter.access$000(MethodSelectingRouter.java:73)
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter$4.apply(MethodSelectingRouter.java:674)
	at org.glassfish.jersey.server.internal.routing.MethodSelectingRouter.apply(MethodSelectingRouter.java:305)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:86)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage._apply(RoutingStage.java:89)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:69)
	at org.glassfish.jersey.server.internal.routing.RoutingStage.apply(RoutingStage.java:38)
	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:173)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:247)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:386)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2022-08-13 12:13:20,603] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:13:21,544] ERROR Uncaught exception in REST call to /connector-plugins/connector1/config/validate (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
java.lang.NullPointerException: Cannot invoke "java.util.Map.get(Object)" because "connectorConfig" is null
	at org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource.validateConfigs(ConnectorPluginsResource.java:77)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:219)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:386)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2022-08-13 12:13:30,604] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:13:34,522] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 12:13:34,523] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:13:34,524] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 12:13:40,609] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:13:50,619] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:14:00,628] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:14:10,639] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:14:20,644] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:14:26,355] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 12:14:26,358] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:14:26,358] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 12:14:26,358] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 12:14:26,359] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 12:14:26,359] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 12:14:26,360] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 12:14:26,360] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:14:26,361] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload.after:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:14:26,361] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 12:14:26,361] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 12:14:26,362] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:14:26,362] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload.after:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:14:26,363] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 12:14:26,681] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:14:26,682] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 12:14:26,746] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 12:14:26,748] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1588977, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 12:14:26,752] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:14:26,759] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:14:26,765] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:14:26,765] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:14:26,766] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:14:26,766] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:14:26,767] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:14:26,769] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:14:26,770] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:14:26,770] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:14:26,771] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:14:26,774] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 12:14:26,775] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 12:14:26,776] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload.after:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:14:26,782] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 12:14:26,783] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 12:14:26,783] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:14:26,783] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:14:26,783] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 12:14:26,783] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 12:14:26,784] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 12:14:26,785] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:14:26,786] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload.after:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:14:26,787] WARN [connector1|task-0] Configuration key whitelist is deprecated and may be removed in the future.  Please update your configuration to use include instead. (org.apache.kafka.common.utils.ConfigUtils:113)
[2022-08-13 12:14:26,787] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.Flatten$Value, org.apache.kafka.connect.transforms.ReplaceField$Value, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 12:14:26,788] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:14:26,793] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 12:14:26,798] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:14:26,798] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:14:26,799] INFO [connector1|task-0] Kafka startTimeMs: 1660373066798 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:14:26,800] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 12:14:26,800] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,800] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,800] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,800] INFO [connector1|task-0]    transforms = t1,t2,t3 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,800] INFO [connector1|task-0]    transforms.t1.type = org.apache.kafka.connect.transforms.Flatten$Value (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,801] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,801] INFO [connector1|task-0]    transforms.t3.whitelist = my_payload (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,801] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,801] INFO [connector1|task-0]    transforms.t2.renames = payload.after:my_payload (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,801] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,801] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:14:26,801] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,801] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,802] INFO [connector1|task-0]    transforms.t2.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,802] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,802] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,802] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,802] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,802] INFO [connector1|task-0]    transformation = or conversion errors (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,802] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,802] INFO [connector1|task-0]    transforms.t3.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,802] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,803] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,803] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,803] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,803] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,803] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,803] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:14:26,815] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1588680, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 12:14:26,821] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 12:14:26,822] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 12:14:26,822] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 12:14:26,823] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:14:26,829] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:14:26,831] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:14:26,832] INFO [connector1|task-0] Kafka startTimeMs: 1660373066829 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:14:26,832] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 12:14:26,833] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:14:26,833] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:14:26,834] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:14:26,838] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:14:26,839] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:14:26,839] INFO [connector1|task-0] Kafka startTimeMs: 1660373066838 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:14:26,845] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:14:26,850] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:14:26,850] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:14:26,851] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:14:26,851] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:14:26,851] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:14:26,852] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:14:26,853] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:14:26,857] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:14:26,857] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:14:26,857] INFO [connector1|task-0] Kafka startTimeMs: 1660373066857 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:14:26,857] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 12:14:26,859] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 12:14:26,862] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:14:26,862] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:14:26,863] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:14:26,863] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:14:26,863] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:14:26,863] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:14:26,864] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:14:26,864] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:14:26,863] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:14:26,864] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:14:26,864] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:14:26,864] INFO [connector1|task-0] Kafka startTimeMs: 1660373066864 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:14:26,869] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:14:26,869] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:14:26,869] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:14:26,869] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:14:26,870] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:14:26,871] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:14:26,871] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:14:26,877] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 12:14:26,883] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:14:26,884] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:14:26,886] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:14:26,886] INFO [connector1|task-0] Kafka startTimeMs: 1660373066884 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:14:26,886] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:14:26,886] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:14:26,886] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:14:26,889] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:14:26,894] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:14:26,895] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:14:26,895] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:14:26,896] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:14:26,896] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:14:26,897] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:14:26,897] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:14:26,902] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:14:26,902] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:14:26,902] INFO [connector1|task-0] Kafka startTimeMs: 1660373066902 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:14:26,912] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:14:26,912] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:14:26,917] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:14:26,917] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:14:26,917] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:14:26,917] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:14:26,917] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:14:26,919] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:14:26,919] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 12:14:26,919] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:14:26,923] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:14:26,923] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:14:26,923] INFO [connector1|task-0] Kafka startTimeMs: 1660373066922 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:14:26,923] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 12:14:26,927] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:14:26,928] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:14:26,932] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 12:14:26,933] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:14:26,937] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:14:26,941] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:14:26,943] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=1, memberId='order_server-dbhistory-9e0b0f74-2499-411e-aec5-8a302fc41643', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 12:14:26,944] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 1: {order_server-dbhistory-9e0b0f74-2499-411e-aec5-8a302fc41643=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 12:14:26,946] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=1, memberId='order_server-dbhistory-9e0b0f74-2499-411e-aec5-8a302fc41643', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 12:14:26,947] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 12:14:26,947] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 12:14:26,949] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 12:14:26,950] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 12:14:26,986] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 12:14:26,987] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-9e0b0f74-2499-411e-aec5-8a302fc41643 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 12:14:26,989] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:14:26,989] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:14:26,994] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:14:26,994] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:14:26,994] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:14:26,995] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:14:26,995] INFO [connector1|task-0] Finished database history recovery of 15 change(s) in 76 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 12:14:26,996] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 12:14:27,002] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 12:14:27,004] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 12:14:27,005] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 12:14:27,005] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 12:14:27,005] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 12:14:27,006] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 12:14:27,006] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 12:14:27,006] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 12:14:27,006] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 12:14:27,006] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1588680, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1588680, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 12:14:27,007] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 12:14:27,007] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 12:14:27,011] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 12:14:27,011] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 12:14:27,011] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:14:27,012] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:14:27,019] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1588680, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1588680, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 12:14:27,019] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 12:14:27,019] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:14:27,120] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 12:14:36,809] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:14:46,819] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:14:48,437] INFO [connector1|task-0] 3 records sent during previous 00:00:21.653, last recorded offset: {transaction_id=null, ts_sec=1660373088, file=SF-CPU-562-bin.000076, pos=1589812, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 12:14:48,467] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:14:56,830] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:15:06,850] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:15:16,865] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:15:26,880] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:15:36,896] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:15:46,897] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:15:56,898] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:16:06,905] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:16:16,914] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:16:26,925] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:16:36,927] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:16:42,559] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:42,560] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:42,561] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:42,576] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cancelled in-flight METADATA request with correlation id 6 due to node 0 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient:341)
[2022-08-13 12:16:44,627] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:44,629] WARN [connector1|task-0] [Producer clientId=order_server-dbhistory] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:44,718] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:44,718] WARN [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:46,788] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:46,788] WARN [connector1|task-0] [Producer clientId=order_server-dbhistory] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:46,881] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:46,881] WARN [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:46,942] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:16:48,935] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:48,936] WARN [connector1|task-0] [Producer clientId=order_server-dbhistory] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:49,152] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:49,152] WARN [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:51,216] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:51,216] WARN [connector1|task-0] [Producer clientId=order_server-dbhistory] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:51,559] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:51,560] WARN [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:53,734] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:53,734] WARN [connector1|task-0] [Producer clientId=order_server-dbhistory] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:54,463] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:54,464] WARN [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:56,759] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:56,760] WARN [connector1|task-0] [Producer clientId=order_server-dbhistory] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:56,946] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:16:57,568] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:57,569] WARN [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:16:59,730] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:16:59,731] WARN [connector1|task-0] [Producer clientId=order_server-dbhistory] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:17:00,479] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:17:00,479] WARN [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:17:02,916] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:17:02,916] WARN [connector1|task-0] [Producer clientId=order_server-dbhistory] Connection to node 0 (host.docker.internal/192.168.0.111:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:775)
[2022-08-13 12:17:03,372] WARN [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Error while fetching metadata with correlation id 8 : {order_server.orders.outbox=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1099)
[2022-08-13 12:17:03,484] WARN [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Error while fetching metadata with correlation id 9 : {order_server.orders.outbox=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1099)
[2022-08-13 12:17:03,591] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:17:06,953] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:17:09,076] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2022-08-13 12:17:09,080] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:311)
[2022-08-13 12:17:09,082] INFO Stopped http_8083@66434cc8{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2022-08-13 12:17:09,083] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2022-08-13 12:17:09,083] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:328)
[2022-08-13 12:17:09,083] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:106)
[2022-08-13 12:17:09,084] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 12:17:09,453] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:17:09,454] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 12:17:09,563] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 12:17:09,563] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660373088, file=SF-CPU-562-bin.000076, pos=1590109, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 12:17:09,569] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:17:09,570] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:17:09,574] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:09,575] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:09,576] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:09,577] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:09,579] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:17:09,580] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:09,581] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:09,581] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:09,581] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:09,582] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 12:17:09,582] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 12:17:09,582] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 12:17:09,583] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:199)
[2022-08-13 12:17:09,583] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2022-08-13 12:17:09,583] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:09,583] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:09,584] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:09,584] INFO App info kafka.connect for 192.168.0.111:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:09,584] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:220)
[2022-08-13 12:17:09,584] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:124)
[2022-08-13 12:17:09,585] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2022-08-13 12:17:18,112] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:68)
[2022-08-13 12:17:18,119] INFO WorkerInfo values: 
	jvm.args = -Xmx256M, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=C:\kafka/logs, -Dlog4j.configuration=file:C:\kafka/config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 17.0.2, 17.0.2+8-LTS-86
	jvm.classpath = C:\kafka\libs\activation-1.1.1.jar;C:\kafka\libs\aopalliance-repackaged-2.6.1.jar;C:\kafka\libs\argparse4j-0.7.0.jar;C:\kafka\libs\audience-annotations-0.5.0.jar;C:\kafka\libs\commons-cli-1.4.jar;C:\kafka\libs\commons-lang3-3.8.1.jar;C:\kafka\libs\connect-api-3.1.0.jar;C:\kafka\libs\connect-basic-auth-extension-3.1.0.jar;C:\kafka\libs\connect-file-3.1.0.jar;C:\kafka\libs\connect-json-3.1.0.jar;C:\kafka\libs\connect-mirror-3.1.0.jar;C:\kafka\libs\connect-mirror-client-3.1.0.jar;C:\kafka\libs\connect-runtime-3.1.0.jar;C:\kafka\libs\connect-transforms-3.1.0.jar;C:\kafka\libs\hk2-api-2.6.1.jar;C:\kafka\libs\hk2-locator-2.6.1.jar;C:\kafka\libs\hk2-utils-2.6.1.jar;C:\kafka\libs\jackson-annotations-2.12.3.jar;C:\kafka\libs\jackson-core-2.12.3.jar;C:\kafka\libs\jackson-databind-2.12.3.jar;C:\kafka\libs\jackson-dataformat-csv-2.12.3.jar;C:\kafka\libs\jackson-datatype-jdk8-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-base-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-json-provider-2.12.3.jar;C:\kafka\libs\jackson-module-jaxb-annotations-2.12.3.jar;C:\kafka\libs\jackson-module-scala_2.13-2.12.3.jar;C:\kafka\libs\jakarta.activation-api-1.2.1.jar;C:\kafka\libs\jakarta.annotation-api-1.3.5.jar;C:\kafka\libs\jakarta.inject-2.6.1.jar;C:\kafka\libs\jakarta.validation-api-2.0.2.jar;C:\kafka\libs\jakarta.ws.rs-api-2.1.6.jar;C:\kafka\libs\jakarta.xml.bind-api-2.3.2.jar;C:\kafka\libs\javassist-3.27.0-GA.jar;C:\kafka\libs\javax.servlet-api-3.1.0.jar;C:\kafka\libs\javax.ws.rs-api-2.1.1.jar;C:\kafka\libs\jaxb-api-2.3.0.jar;C:\kafka\libs\jersey-client-2.34.jar;C:\kafka\libs\jersey-common-2.34.jar;C:\kafka\libs\jersey-container-servlet-2.34.jar;C:\kafka\libs\jersey-container-servlet-core-2.34.jar;C:\kafka\libs\jersey-hk2-2.34.jar;C:\kafka\libs\jersey-server-2.34.jar;C:\kafka\libs\jetty-client-9.4.43.v20210629.jar;C:\kafka\libs\jetty-continuation-9.4.43.v20210629.jar;C:\kafka\libs\jetty-http-9.4.43.v20210629.jar;C:\kafka\libs\jetty-io-9.4.43.v20210629.jar;C:\kafka\libs\jetty-security-9.4.43.v20210629.jar;C:\kafka\libs\jetty-server-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlet-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlets-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-ajax-9.4.43.v20210629.jar;C:\kafka\libs\jline-3.12.1.jar;C:\kafka\libs\jopt-simple-5.0.4.jar;C:\kafka\libs\jose4j-0.7.8.jar;C:\kafka\libs\kafka-clients-3.1.0.jar;C:\kafka\libs\kafka-log4j-appender-3.1.0.jar;C:\kafka\libs\kafka-metadata-3.1.0.jar;C:\kafka\libs\kafka-raft-3.1.0.jar;C:\kafka\libs\kafka-server-common-3.1.0.jar;C:\kafka\libs\kafka-shell-3.1.0.jar;C:\kafka\libs\kafka-storage-3.1.0.jar;C:\kafka\libs\kafka-storage-api-3.1.0.jar;C:\kafka\libs\kafka-streams-3.1.0.jar;C:\kafka\libs\kafka-streams-examples-3.1.0.jar;C:\kafka\libs\kafka-streams-scala_2.13-3.1.0.jar;C:\kafka\libs\kafka-streams-test-utils-3.1.0.jar;C:\kafka\libs\kafka-tools-3.1.0.jar;C:\kafka\libs\kafka_2.13-3.1.0.jar;C:\kafka\libs\log4j-1.2.17.jar;C:\kafka\libs\lz4-java-1.8.0.jar;C:\kafka\libs\maven-artifact-3.8.1.jar;C:\kafka\libs\metrics-core-2.2.0.jar;C:\kafka\libs\metrics-core-4.1.12.1.jar;C:\kafka\libs\netty-buffer-4.1.68.Final.jar;C:\kafka\libs\netty-codec-4.1.68.Final.jar;C:\kafka\libs\netty-common-4.1.68.Final.jar;C:\kafka\libs\netty-handler-4.1.68.Final.jar;C:\kafka\libs\netty-resolver-4.1.68.Final.jar;C:\kafka\libs\netty-transport-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-epoll-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-unix-common-4.1.68.Final.jar;C:\kafka\libs\osgi-resource-locator-1.0.3.jar;C:\kafka\libs\paranamer-2.8.jar;C:\kafka\libs\plexus-utils-3.2.1.jar;C:\kafka\libs\reflections-0.9.12.jar;C:\kafka\libs\rocksdbjni-6.22.1.1.jar;C:\kafka\libs\scala-collection-compat_2.13-2.4.4.jar;C:\kafka\libs\scala-java8-compat_2.13-1.0.0.jar;C:\kafka\libs\scala-library-2.13.6.jar;C:\kafka\libs\scala-logging_2.13-3.9.3.jar;C:\kafka\libs\scala-reflect-2.13.6.jar;C:\kafka\libs\slf4j-api-1.7.30.jar;C:\kafka\libs\slf4j-log4j12-1.7.30.jar;C:\kafka\libs\snappy-java-1.1.8.4.jar;C:\kafka\libs\trogdor-3.1.0.jar;C:\kafka\libs\zookeeper-3.6.3.jar;C:\kafka\libs\zookeeper-jute-3.6.3.jar;C:\kafka\libs\zstd-jni-1.5.0-4.jar
	os.spec = Windows 10, amd64, 10.0
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2022-08-13 12:17:18,121] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:77)
[2022-08-13 12:17:18,132] INFO Loading plugin from: C:\kafka\opt\connectors\debezium-debezium-connector-mysql-1.9.3 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:265)
[2022-08-13 12:17:18,521] INFO Registered loader: PluginClassLoader{pluginLocation=file:/C:/kafka/opt/connectors/debezium-debezium-connector-mysql-1.9.3/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 12:17:18,522] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,523] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,523] INFO Added plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,523] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,523] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,524] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,524] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,524] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,524] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,524] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,524] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,525] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:18,525] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,283] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@266474c2 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 12:17:19,283] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,285] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,285] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,286] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,286] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,286] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,286] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,286] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,286] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,287] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,287] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,287] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,287] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,287] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,287] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,287] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,287] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,288] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,288] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,288] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,288] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,288] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,288] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,288] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,289] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,289] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,289] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,289] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,289] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,289] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,289] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,289] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,290] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,290] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,290] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,290] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,290] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,290] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,290] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,290] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,290] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,291] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,291] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,291] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,291] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,291] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,291] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,292] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,292] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,292] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,292] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,292] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,292] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 12:17:19,293] INFO Added aliases 'MySqlConnector' and 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,293] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,293] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,293] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,293] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,293] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,293] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,294] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,294] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,294] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,294] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,294] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,294] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,294] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,295] INFO Added aliases 'CloudEventsConverter' and 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,295] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,295] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,295] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,295] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,295] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,295] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,295] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,296] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,296] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,296] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,296] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,296] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,296] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,296] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,296] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,297] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,297] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,297] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,297] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,297] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,297] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,297] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,298] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,298] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,298] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,298] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,298] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,298] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,298] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,298] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,299] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,304] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,304] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 12:17:19,305] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,305] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,305] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 12:17:19,318] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = All
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = C:/kafka/tmp/connect.offsets
	plugin.path = [/opt/connectors, C:/kafka/opt/connectors]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:376)
[2022-08-13 12:17:19,319] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 12:17:19,322] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 12:17:19,400] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,401] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,402] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,402] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,402] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,402] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,403] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,403] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:19,403] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:19,403] INFO Kafka startTimeMs: 1660373239403 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:19,635] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 12:17:19,636] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:19,641] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:19,641] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:19,642] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:19,650] INFO Logging initialized @1887ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2022-08-13 12:17:19,680] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:117)
[2022-08-13 12:17:19,681] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:188)
[2022-08-13 12:17:19,685] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 17.0.2+8-LTS-86 (org.eclipse.jetty.server.Server:375)
[2022-08-13 12:17:19,703] INFO Started http_8083@66434cc8{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2022-08-13 12:17:19,704] INFO Started @1941ms (org.eclipse.jetty.server.Server:415)
[2022-08-13 12:17:19,720] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 12:17:19,720] INFO REST server listening at http://192.168.0.111:8083/, advertising URL http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2022-08-13 12:17:19,721] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 12:17:19,721] INFO REST admin endpoints at http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2022-08-13 12:17:19,721] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 12:17:19,721] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:44)
[2022-08-13 12:17:19,726] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 12:17:19,727] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 12:17:19,732] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,732] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,732] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,733] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,733] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,733] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,733] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:19,733] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:19,733] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:19,734] INFO Kafka startTimeMs: 1660373239733 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:19,746] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 12:17:19,748] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:19,750] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:19,751] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:19,751] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:19,755] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:19,755] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:19,755] INFO Kafka startTimeMs: 1660373239755 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:19,839] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:17:19,842] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:17:19,853] INFO Kafka Connect standalone worker initialization took 1738ms (org.apache.kafka.connect.cli.ConnectStandalone:99)
[2022-08-13 12:17:19,853] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2022-08-13 12:17:19,854] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:98)
[2022-08-13 12:17:19,854] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:185)
[2022-08-13 12:17:19,854] INFO Starting FileOffsetBackingStore with file C:\kafka\tmp\connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2022-08-13 12:17:19,862] INFO Worker started (org.apache.kafka.connect.runtime.Worker:192)
[2022-08-13 12:17:19,862] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:101)
[2022-08-13 12:17:19,862] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:208)
[2022-08-13 12:17:19,887] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:225)
[2022-08-13 12:17:19,930] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2022-08-13 12:17:19,930] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2022-08-13 12:17:19,932] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2022-08-13 12:17:20,254] INFO Started o.e.j.s.ServletContextHandler@221dad51{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:915)
[2022-08-13 12:17:20,255] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2022-08-13 12:17:20,255] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2022-08-13 12:17:20,433] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 12:17:20,439] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:17:20,440] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 12:17:20,447] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 12:17:20,448] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:17:20,459] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:17:20,462] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 12:17:20,462] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 12:17:20,465] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:17:20,466] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:17:20,468] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 12:17:20,469] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 12:17:20,469] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:17:20,471] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 12:17:20,471] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 12:17:20,472] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:17:20,472] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:17:20,472] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 12:17:20,472] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 12:17:20,473] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 12:17:20,475] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:17:20,475] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:17:20,476] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 12:17:20,480] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:17:20,500] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 12:17:20,500] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:20,501] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:20,501] INFO [connector1|task-0] Kafka startTimeMs: 1660373240500 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:20,507] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:20,508] INFO Created connector connector1 (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2022-08-13 12:17:20,508] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 12:17:20,509] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,509] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,509] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,509] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,509] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,510] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,512] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,512] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,512] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,512] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,512] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,512] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,512] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,513] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,513] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,513] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,513] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,513] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,513] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,513] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:20,566] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1589812, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 12:17:20,593] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 12:17:20,593] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 12:17:20,594] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 12:17:20,595] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:17:20,600] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:20,600] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:20,600] INFO [connector1|task-0] Kafka startTimeMs: 1660373240600 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:20,604] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:20,620] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 12:17:20,621] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:17:20,628] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:17:20,657] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:20,658] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:20,659] INFO [connector1|task-0] Kafka startTimeMs: 1660373240657 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:20,665] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:20,671] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:17:20,672] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:20,672] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:20,672] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:20,672] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:20,673] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:20,674] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:17:20,678] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:20,679] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:20,679] INFO [connector1|task-0] Kafka startTimeMs: 1660373240678 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:20,679] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 12:17:20,680] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 12:17:20,685] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:20,688] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:20,688] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:20,688] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:20,688] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:20,688] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:20,689] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:20,689] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:20,689] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:20,689] INFO [connector1|task-0] Kafka startTimeMs: 1660373240689 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:20,689] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:17:20,690] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:20,710] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:17:20,710] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:20,711] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:20,711] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:20,711] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:20,712] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:20,713] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:17:20,718] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:20,721] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:20,721] INFO [connector1|task-0] Kafka startTimeMs: 1660373240718 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:20,724] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 12:17:20,724] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:20,725] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:20,726] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:20,726] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:20,726] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:20,728] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:17:20,728] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:20,728] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:20,728] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:20,728] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:20,729] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:20,730] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:17:20,746] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:20,755] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:20,755] INFO [connector1|task-0] Kafka startTimeMs: 1660373240746 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:20,761] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:17:20,761] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:20,768] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:17:20,769] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:20,769] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:20,769] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:20,769] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:20,771] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:20,771] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 12:17:20,777] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:17:20,783] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:20,785] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:20,785] INFO [connector1|task-0] Kafka startTimeMs: 1660373240783 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:20,785] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 12:17:20,789] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:17:20,789] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:20,794] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 12:17:20,797] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:17:20,810] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:20,811] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:17:20,819] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=1, memberId='order_server-dbhistory-d3358eaf-c615-4162-97b0-88f6d1fd1835', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 12:17:20,821] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 1: {order_server-dbhistory-d3358eaf-c615-4162-97b0-88f6d1fd1835=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 12:17:20,858] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=1, memberId='order_server-dbhistory-d3358eaf-c615-4162-97b0-88f6d1fd1835', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 12:17:20,859] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 12:17:20,861] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 12:17:20,871] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 12:17:20,877] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 12:17:21,295] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 12:17:21,296] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-d3358eaf-c615-4162-97b0-88f6d1fd1835 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 12:17:21,298] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:17:21,299] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:21,310] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:21,310] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:21,310] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:21,312] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:21,312] INFO [connector1|task-0] Finished database history recovery of 15 change(s) in 540 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 12:17:21,325] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 12:17:21,328] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 12:17:21,330] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 12:17:21,354] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 12:17:21,355] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 12:17:21,356] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 12:17:21,356] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 12:17:21,358] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 12:17:21,358] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 12:17:21,361] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 12:17:21,362] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1589812, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1589812, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 12:17:21,365] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 12:17:21,370] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 12:17:21,377] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 12:17:21,378] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 12:17:21,378] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:17:21,384] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:17:21,394] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1589812, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1589812, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 12:17:21,394] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 12:17:21,395] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:17:21,506] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 12:17:30,514] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:17:36,136] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 12:17:36,137] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:17:36,138] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 12:17:36,139] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 12:17:36,139] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 12:17:36,140] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 12:17:36,140] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 12:17:36,141] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:17:36,141] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload.after:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:17:36,141] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 12:17:36,142] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 12:17:36,142] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:17:36,143] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload.after:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:17:36,143] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 12:17:36,186] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:17:36,186] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 12:17:36,281] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 12:17:36,283] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1590109, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 12:17:36,288] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:17:36,298] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:17:36,304] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:36,304] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:36,305] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:36,305] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:36,306] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:17:36,308] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:36,309] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:36,309] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:36,309] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:36,313] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 12:17:36,315] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 12:17:36,315] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload.after:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:17:36,316] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 12:17:36,316] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 12:17:36,317] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:17:36,317] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:17:36,317] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 12:17:36,317] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 12:17:36,318] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 12:17:36,319] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:17:36,320] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload.after:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:17:36,321] WARN [connector1|task-0] Configuration key whitelist is deprecated and may be removed in the future.  Please update your configuration to use include instead. (org.apache.kafka.common.utils.ConfigUtils:113)
[2022-08-13 12:17:36,321] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.Flatten$Value, org.apache.kafka.connect.transforms.ReplaceField$Value, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 12:17:36,321] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:17:36,325] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 12:17:36,325] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:36,325] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:36,325] INFO [connector1|task-0] Kafka startTimeMs: 1660373256325 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:36,329] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:36,333] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 12:17:36,333] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,333] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,333] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,334] INFO [connector1|task-0]    transforms = t1,t2,t3 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,334] INFO [connector1|task-0]    transforms.t1.type = org.apache.kafka.connect.transforms.Flatten$Value (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,334] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,334] INFO [connector1|task-0]    transforms.t3.whitelist = my_payload (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,334] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,335] INFO [connector1|task-0]    transforms.t2.renames = payload.after:my_payload (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,335] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,335] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,335] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,335] INFO [connector1|task-0]    transforms.t2.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,335] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,335] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,336] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,336] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,336] INFO [connector1|task-0]    transformation = or conversion errors (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,336] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,336] INFO [connector1|task-0]    transforms.t3.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,336] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,336] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,336] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,337] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,337] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,337] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,337] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:17:36,347] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1589812, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 12:17:36,354] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 12:17:36,355] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 12:17:36,355] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 12:17:36,359] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:17:36,363] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:36,364] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:36,364] INFO [connector1|task-0] Kafka startTimeMs: 1660373256363 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:36,364] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 12:17:36,365] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:17:36,366] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:17:36,368] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:36,371] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:36,371] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:36,371] INFO [connector1|task-0] Kafka startTimeMs: 1660373256370 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:36,376] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:36,381] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:17:36,381] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:36,381] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:36,382] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:36,382] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:36,383] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:36,383] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:17:36,389] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:36,390] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:36,390] INFO [connector1|task-0] Kafka startTimeMs: 1660373256389 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:36,390] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 12:17:36,391] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 12:17:36,395] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:36,395] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:17:36,396] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:36,396] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:36,396] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:36,397] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:36,398] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:36,398] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:36,398] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:17:36,398] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:36,399] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:36,399] INFO [connector1|task-0] Kafka startTimeMs: 1660373256398 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:36,404] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:17:36,411] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:36,411] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:36,412] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:36,412] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:36,414] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:36,415] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:17:36,417] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 12:17:36,417] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:36,419] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:36,419] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:36,419] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:36,420] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:36,420] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:36,420] INFO [connector1|task-0] Kafka startTimeMs: 1660373256420 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:36,424] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:36,427] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:17:36,427] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:36,427] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:36,427] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:36,427] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:36,428] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:36,429] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:17:36,433] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:36,433] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:36,433] INFO [connector1|task-0] Kafka startTimeMs: 1660373256433 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:36,437] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:17:36,441] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:36,444] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:17:36,445] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:36,445] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:36,445] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:36,445] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:36,446] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:36,446] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 12:17:36,447] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:17:36,450] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:17:36,450] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:17:36,456] INFO [connector1|task-0] Kafka startTimeMs: 1660373256450 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:17:36,456] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 12:17:36,460] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:17:36,460] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:17:36,465] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 12:17:36,466] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:17:36,471] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:36,472] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:17:36,475] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=3, memberId='order_server-dbhistory-0894c448-0332-4cfa-92fc-749fdd72b827', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 12:17:36,475] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 3: {order_server-dbhistory-0894c448-0332-4cfa-92fc-749fdd72b827=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 12:17:36,478] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=3, memberId='order_server-dbhistory-0894c448-0332-4cfa-92fc-749fdd72b827', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 12:17:36,478] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 12:17:36,479] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 12:17:36,481] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 12:17:36,489] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 12:17:36,526] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 12:17:36,526] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-0894c448-0332-4cfa-92fc-749fdd72b827 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 12:17:36,528] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:17:36,528] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:17:36,533] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:17:36,533] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:17:36,533] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:17:36,535] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:17:36,535] INFO [connector1|task-0] Finished database history recovery of 15 change(s) in 89 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 12:17:36,536] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 12:17:36,539] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 12:17:36,540] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 12:17:36,541] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 12:17:36,541] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 12:17:36,541] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 12:17:36,541] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 12:17:36,543] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 12:17:36,543] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 12:17:36,543] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 12:17:36,543] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1589812, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1589812, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 12:17:36,544] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 12:17:36,544] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 12:17:36,549] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 12:17:36,563] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 12:17:36,563] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:17:36,566] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:17:36,570] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1589812, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1589812, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 12:17:36,571] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 12:17:36,571] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:17:36,672] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 12:17:46,337] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:17:56,346] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:18:06,362] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:18:14,244] INFO [connector1|task-0] 3 records sent during previous 00:00:37.927, last recorded offset: {transaction_id=null, ts_sec=1660373293, file=SF-CPU-562-bin.000076, pos=1590944, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 12:18:14,271] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:18:16,370] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:18:26,385] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:18:36,387] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:18:46,391] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:18:52,070] INFO [connector1|task-0] 3 records sent during previous 00:00:37.826, last recorded offset: {transaction_id=null, ts_sec=1660373331, file=SF-CPU-562-bin.000076, pos=1592076, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 12:18:56,399] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:19:06,418] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:19:16,430] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:19:26,442] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:19:36,456] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:19:46,472] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:19:56,473] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:20:06,490] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:20:16,491] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:20:25,716] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 12:20:25,718] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:20:25,719] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 12:20:25,719] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 12:20:25,719] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 12:20:25,720] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 12:20:25,720] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 12:20:25,720] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:20:25,721] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:20:25,722] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 12:20:25,722] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 12:20:25,722] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:20:25,723] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:20:25,724] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 12:20:26,083] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:20:26,084] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 12:20:26,193] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660373338, file=SF-CPU-562-bin.000076, pos=1593505, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 12:20:26,193] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 12:20:26,200] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:20:26,201] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:20:26,205] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:20:26,206] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:20:26,207] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:20:26,208] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:20:26,213] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:20:26,217] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:20:26,218] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:20:26,218] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:20:26,219] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:20:26,220] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 12:20:26,221] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 12:20:26,222] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:20:26,223] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 12:20:26,223] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 12:20:26,224] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:20:26,232] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:20:26,232] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 12:20:26,233] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 12:20:26,233] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 12:20:26,235] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:20:26,235] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [t1, t2, t3]
	transforms.t1.delimiter = .
	transforms.t1.negate = false
	transforms.t1.predicate = 
	transforms.t1.type = class org.apache.kafka.connect.transforms.Flatten$Value
	transforms.t2.blacklist = null
	transforms.t2.exclude = []
	transforms.t2.include = []
	transforms.t2.negate = false
	transforms.t2.predicate = 
	transforms.t2.renames = [payload:my_payload]
	transforms.t2.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t2.whitelist = null
	transforms.t3.blacklist = null
	transforms.t3.exclude = []
	transforms.t3.include = []
	transforms.t3.negate = false
	transforms.t3.predicate = 
	transforms.t3.renames = []
	transforms.t3.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.t3.whitelist = [my_payload]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:20:26,237] WARN [connector1|task-0] Configuration key whitelist is deprecated and may be removed in the future.  Please update your configuration to use include instead. (org.apache.kafka.common.utils.ConfigUtils:113)
[2022-08-13 12:20:26,237] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.Flatten$Value, org.apache.kafka.connect.transforms.ReplaceField$Value, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 12:20:26,238] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:20:26,242] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 12:20:26,246] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:20:26,246] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:20:26,246] INFO [connector1|task-0] Kafka startTimeMs: 1660373426246 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:20:26,247] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 12:20:26,248] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,248] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,248] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,248] INFO [connector1|task-0]    transforms = t1,t2,t3 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,248] INFO [connector1|task-0]    transforms.t1.type = org.apache.kafka.connect.transforms.Flatten$Value (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,248] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,248] INFO [connector1|task-0]    transforms.t3.whitelist = my_payload (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,248] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,248] INFO [connector1|task-0]    transforms.t2.renames = payload:my_payload (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,249] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,249] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,249] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,249] INFO [connector1|task-0]    transforms.t2.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,249] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,250] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,250] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:20:26,250] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,250] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,251] INFO [connector1|task-0]    transformation = or conversion errors (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,251] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,251] INFO [connector1|task-0]    transforms.t3.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,251] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,251] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,252] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,252] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,252] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,252] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,252] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:20:26,262] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1593208, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 12:20:26,267] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 12:20:26,268] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 12:20:26,268] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 12:20:26,269] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:20:26,273] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:20:26,277] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:20:26,277] INFO [connector1|task-0] Kafka startTimeMs: 1660373426273 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:20:26,278] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 12:20:26,278] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:20:26,279] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:20:26,281] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:20:26,284] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:20:26,285] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:20:26,285] INFO [connector1|task-0] Kafka startTimeMs: 1660373426284 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:20:26,290] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:20:26,296] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:20:26,297] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:20:26,297] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:20:26,297] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:20:26,297] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:20:26,298] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:20:26,299] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:20:26,304] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:20:26,305] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:20:26,305] INFO [connector1|task-0] Kafka startTimeMs: 1660373426304 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:20:26,305] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 12:20:26,306] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 12:20:26,309] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:20:26,309] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:20:26,310] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:20:26,310] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:20:26,310] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:20:26,310] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:20:26,310] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:20:26,310] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:20:26,310] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:20:26,311] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:20:26,311] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:20:26,311] INFO [connector1|task-0] Kafka startTimeMs: 1660373426311 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:20:26,316] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:20:26,324] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 12:20:26,325] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:20:26,325] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:20:26,325] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:20:26,325] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:20:26,325] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:20:26,327] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:20:26,327] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:20:26,327] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:20:26,327] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:20:26,328] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:20:26,332] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:20:26,332] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:20:26,332] INFO [connector1|task-0] Kafka startTimeMs: 1660373426332 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:20:26,337] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:20:26,338] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:20:26,338] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:20:26,338] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:20:26,338] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:20:26,339] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:20:26,339] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:20:26,340] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:20:26,344] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:20:26,345] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:20:26,345] INFO [connector1|task-0] Kafka startTimeMs: 1660373426344 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:20:26,350] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:20:26,356] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:20:26,360] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:20:26,361] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:20:26,361] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:20:26,361] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:20:26,361] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:20:26,362] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:20:26,362] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 12:20:26,363] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:20:26,366] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:20:26,367] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:20:26,367] INFO [connector1|task-0] Kafka startTimeMs: 1660373426366 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:20:26,367] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 12:20:26,371] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:20:26,372] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:20:26,375] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 12:20:26,376] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:20:26,379] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:20:26,390] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:20:26,392] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=5, memberId='order_server-dbhistory-73a29260-a531-48c2-8c16-57601e37c7c8', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 12:20:26,392] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 5: {order_server-dbhistory-73a29260-a531-48c2-8c16-57601e37c7c8=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 12:20:26,395] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=5, memberId='order_server-dbhistory-73a29260-a531-48c2-8c16-57601e37c7c8', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 12:20:26,395] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 12:20:26,396] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 12:20:26,397] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 12:20:26,400] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 12:20:26,422] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 12:20:26,423] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-73a29260-a531-48c2-8c16-57601e37c7c8 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 12:20:26,423] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:20:26,423] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:20:26,428] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:20:26,428] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:20:26,429] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:20:26,429] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:20:26,430] INFO [connector1|task-0] Finished database history recovery of 15 change(s) in 67 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 12:20:26,430] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 12:20:26,433] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 12:20:26,435] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 12:20:26,435] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 12:20:26,436] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 12:20:26,436] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 12:20:26,436] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 12:20:26,437] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 12:20:26,437] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 12:20:26,438] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 12:20:26,438] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1593208, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1593208, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 12:20:26,438] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 12:20:26,438] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 12:20:26,441] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 12:20:26,441] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 12:20:26,441] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:20:26,445] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:20:26,452] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1593208, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1593208, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 12:20:26,452] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 12:20:26,452] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:20:26,569] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 12:20:35,636] INFO [connector1|task-0] 3 records sent during previous 00:00:09.412, last recorded offset: {transaction_id=null, ts_sec=1660373435, file=SF-CPU-562-bin.000076, pos=1594340, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 12:20:35,640] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:20:36,261] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:20:46,279] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:20:56,285] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:21:06,294] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:21:10,134] ERROR Uncaught exception in REST call to /connectors/connector1/config (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
com.fasterxml.jackson.core.JsonParseException: Unexpected character ('/' (code 47)): maybe a (non-standard) comment? (not recognized as one since Feature 'ALLOW_COMMENTS' not enabled for parser)
 at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 22, column: 6]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:2337)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:710)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:635)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._skipComment(UTF8StreamJsonParser.java:3165)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._skipWSOrEnd2(UTF8StreamJsonParser.java:3045)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._skipWSOrEnd(UTF8StreamJsonParser.java:3021)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextFieldName(UTF8StreamJsonParser.java:1002)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:594)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:437)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:32)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:322)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:2007)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1174)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:816)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:885)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:290)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:386)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2022-08-13 12:21:16,298] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:21:26,300] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:21:36,313] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:21:43,036] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 12:21:43,037] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:21:43,038] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 12:21:43,038] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 12:21:43,038] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 12:21:43,038] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 12:21:43,039] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 12:21:43,039] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:21:43,039] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:21:43,039] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 12:21:43,040] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 12:21:43,040] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:21:43,040] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:21:43,040] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 12:21:43,105] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:21:43,105] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 12:21:43,181] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 12:21:43,181] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660373435, file=SF-CPU-562-bin.000076, pos=1594637, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 12:21:43,185] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:21:43,185] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:21:43,187] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:21:43,188] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:21:43,188] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:21:43,188] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:21:43,189] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:21:43,192] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:21:43,192] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:21:43,193] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:21:43,193] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:21:43,194] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 12:21:43,195] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 12:21:43,196] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:21:43,196] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 12:21:43,196] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 12:21:43,197] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:21:43,205] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:21:43,206] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 12:21:43,206] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 12:21:43,206] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 12:21:43,206] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:21:43,206] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:21:43,207] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 12:21:43,207] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:21:43,211] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 12:21:43,212] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:21:43,212] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:21:43,212] INFO [connector1|task-0] Kafka startTimeMs: 1660373503211 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:21:43,213] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 12:21:43,213] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,213] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,213] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,214] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,214] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,214] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,214] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,214] INFO [connector1|task-0]    transformation = or conversion errors (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,214] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,214] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,215] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,215] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,215] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,215] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,215] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,214] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:21:43,215] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,216] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,216] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,216] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,216] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,216] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:21:43,223] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1594340, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 12:21:43,228] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 12:21:43,234] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 12:21:43,234] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 12:21:43,235] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:21:43,238] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:21:43,238] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:21:43,238] INFO [connector1|task-0] Kafka startTimeMs: 1660373503238 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:21:43,238] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 12:21:43,239] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:21:43,240] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:21:43,241] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:21:43,246] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:21:43,250] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:21:43,250] INFO [connector1|task-0] Kafka startTimeMs: 1660373503246 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:21:43,254] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:21:43,255] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:21:43,255] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:21:43,256] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:21:43,256] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:21:43,256] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:21:43,257] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:21:43,257] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:21:43,263] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:21:43,267] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:21:43,268] INFO [connector1|task-0] Kafka startTimeMs: 1660373503263 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:21:43,268] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 12:21:43,269] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 12:21:43,272] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:21:43,272] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:21:43,272] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:21:43,273] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:21:43,273] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:21:43,273] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:21:43,273] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:21:43,272] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:21:43,273] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:21:43,274] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:21:43,274] INFO [connector1|task-0] Kafka startTimeMs: 1660373503273 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:21:43,274] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:21:43,280] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:21:43,280] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:21:43,280] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:21:43,280] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:21:43,281] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:21:43,282] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:21:43,282] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:21:43,286] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:21:43,286] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:21:43,286] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 12:21:43,286] INFO [connector1|task-0] Kafka startTimeMs: 1660373503286 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:21:43,287] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:21:43,288] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:21:43,288] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:21:43,289] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:21:43,290] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:21:43,291] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:21:43,301] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:21:43,302] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:21:43,302] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:21:43,302] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:21:43,303] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:21:43,304] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:21:43,307] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:21:43,307] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:21:43,307] INFO [connector1|task-0] Kafka startTimeMs: 1660373503307 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:21:43,312] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:21:43,312] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:21:43,317] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:21:43,317] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:21:43,317] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:21:43,317] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:21:43,317] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:21:43,318] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:21:43,318] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 12:21:43,319] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:21:43,322] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:21:43,322] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:21:43,322] INFO [connector1|task-0] Kafka startTimeMs: 1660373503322 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:21:43,323] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 12:21:43,335] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:21:43,335] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:21:43,340] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 12:21:43,342] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:21:43,346] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:21:43,347] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:21:43,350] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=7, memberId='order_server-dbhistory-b4c80c2e-faec-4506-93ce-4e4a7447ecd3', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 12:21:43,351] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 7: {order_server-dbhistory-b4c80c2e-faec-4506-93ce-4e4a7447ecd3=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 12:21:43,354] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=7, memberId='order_server-dbhistory-b4c80c2e-faec-4506-93ce-4e4a7447ecd3', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 12:21:43,354] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 12:21:43,354] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 12:21:43,356] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 12:21:43,361] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 12:21:43,393] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 12:21:43,393] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-b4c80c2e-faec-4506-93ce-4e4a7447ecd3 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 12:21:43,395] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:21:43,396] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:21:43,399] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:21:43,400] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:21:43,400] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:21:43,401] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:21:43,401] INFO [connector1|task-0] Finished database history recovery of 15 change(s) in 82 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 12:21:43,402] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 12:21:43,405] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 12:21:43,407] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 12:21:43,407] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 12:21:43,407] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 12:21:43,408] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 12:21:43,408] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 12:21:43,409] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 12:21:43,409] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 12:21:43,409] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 12:21:43,410] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1594340, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1594340, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 12:21:43,410] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 12:21:43,410] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 12:21:43,414] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 12:21:43,414] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 12:21:43,414] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:21:43,416] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:21:43,425] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1594340, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1594340, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 12:21:43,426] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 12:21:43,426] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:21:43,527] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 12:21:51,072] INFO [connector1|task-0] 3 records sent during previous 00:00:07.875, last recorded offset: {transaction_id=null, ts_sec=1660373510, file=SF-CPU-562-bin.000076, pos=1595472, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 12:21:51,085] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:21:53,227] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:22:03,237] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:22:13,249] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:22:23,254] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:22:33,258] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:22:43,273] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:22:53,277] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:23:03,281] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:23:13,292] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:23:23,297] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:23:33,300] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:23:43,305] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:23:51,438] ERROR Uncaught exception in REST call to /connectors/connector1/config (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'unwrap': was expecting (JSON String, Number, Array, Object or token 'null', 'true' or 'false')
 at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 22, column: 25]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:2337)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:720)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._reportInvalidToken(UTF8StreamJsonParser.java:3593)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleUnexpectedValue(UTF8StreamJsonParser.java:2688)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextFieldName(UTF8StreamJsonParser.java:1094)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:594)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:437)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:32)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:322)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:2007)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1174)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:816)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:885)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:290)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:386)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2022-08-13 12:23:53,321] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:24:03,334] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:24:09,177] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 12:24:09,179] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:24:09,180] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 12:24:09,180] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 12:24:09,180] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 12:24:09,180] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 12:24:09,181] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 12:24:09,181] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:24:09,181] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:24:09,181] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 12:24:09,182] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 12:24:09,182] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:24:09,182] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:24:09,182] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 12:24:09,195] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:24:09,201] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 12:24:09,289] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 12:24:09,289] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660373510, file=SF-CPU-562-bin.000076, pos=1595769, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 12:24:09,291] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:24:09,291] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:24:09,292] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:24:09,292] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:24:09,292] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:24:09,292] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:24:09,293] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:24:09,294] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:24:09,294] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:24:09,294] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:24:09,294] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:24:09,294] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 12:24:09,295] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 12:24:09,295] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:24:09,295] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 12:24:09,295] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 12:24:09,296] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:24:09,296] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:24:09,296] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 12:24:09,296] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 12:24:09,296] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 12:24:09,296] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:24:09,297] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:24:09,298] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 12:24:09,298] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:24:09,301] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 12:24:09,301] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:24:09,301] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:24:09,301] INFO [connector1|task-0] Kafka startTimeMs: 1660373649301 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:24:09,302] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 12:24:09,302] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,302] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,302] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,302] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    transformation = or conversion errors (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,304] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,303] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:24:09,304] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,304] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,304] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,304] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,304] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:24:09,313] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1595472, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 12:24:09,317] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 12:24:09,317] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 12:24:09,317] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 12:24:09,318] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:24:09,321] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:24:09,328] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:24:09,328] INFO [connector1|task-0] Kafka startTimeMs: 1660373649321 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:24:09,329] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 12:24:09,329] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:24:09,330] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:24:09,331] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:24:09,334] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:24:09,334] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:24:09,334] INFO [connector1|task-0] Kafka startTimeMs: 1660373649334 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:24:09,337] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:24:09,339] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:24:09,339] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:24:09,340] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:24:09,340] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:24:09,340] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:24:09,341] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:24:09,342] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:24:09,344] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:24:09,345] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:24:09,345] INFO [connector1|task-0] Kafka startTimeMs: 1660373649344 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:24:09,345] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 12:24:09,346] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 12:24:09,349] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:24:09,349] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:24:09,349] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:24:09,349] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:24:09,349] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:24:09,350] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:24:09,350] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:24:09,350] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:24:09,350] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:24:09,350] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:24:09,350] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:24:09,350] INFO [connector1|task-0] Kafka startTimeMs: 1660373649350 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:24:09,355] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:24:09,361] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:24:09,361] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:24:09,361] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:24:09,361] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:24:09,362] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:24:09,363] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 12:24:09,363] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:24:09,363] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:24:09,365] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:24:09,365] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:24:09,365] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:24:09,367] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:24:09,367] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:24:09,367] INFO [connector1|task-0] Kafka startTimeMs: 1660373649367 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:24:09,371] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:24:09,373] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:24:09,373] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:24:09,373] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:24:09,374] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:24:09,374] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:24:09,374] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:24:09,375] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:24:09,378] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:24:09,378] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:24:09,378] INFO [connector1|task-0] Kafka startTimeMs: 1660373649378 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:24:09,381] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:24:09,387] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:24:09,392] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:24:09,392] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:24:09,392] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:24:09,392] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:24:09,393] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:24:09,394] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:24:09,394] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 12:24:09,394] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:24:09,399] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:24:09,404] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:24:09,404] INFO [connector1|task-0] Kafka startTimeMs: 1660373649399 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:24:09,405] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 12:24:09,410] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:24:09,410] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:24:09,414] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 12:24:09,415] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:24:09,418] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:24:09,418] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:24:09,421] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=9, memberId='order_server-dbhistory-58401056-2b52-4c35-8a2b-fb86ce8e345f', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 12:24:09,421] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 9: {order_server-dbhistory-58401056-2b52-4c35-8a2b-fb86ce8e345f=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 12:24:09,423] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=9, memberId='order_server-dbhistory-58401056-2b52-4c35-8a2b-fb86ce8e345f', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 12:24:09,424] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 12:24:09,424] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 12:24:09,425] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 12:24:09,427] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 12:24:09,457] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 12:24:09,457] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-58401056-2b52-4c35-8a2b-fb86ce8e345f sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 12:24:09,458] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:24:09,460] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:24:09,466] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:24:09,466] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:24:09,467] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:24:09,468] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:24:09,469] INFO [connector1|task-0] Finished database history recovery of 15 change(s) in 75 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 12:24:09,470] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 12:24:09,474] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 12:24:09,480] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 12:24:09,481] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 12:24:09,481] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 12:24:09,481] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 12:24:09,481] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 12:24:09,482] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 12:24:09,482] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 12:24:09,484] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 12:24:09,485] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1595472, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1595472, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 12:24:09,485] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 12:24:09,486] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 12:24:09,489] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 12:24:09,489] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 12:24:09,505] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:24:09,506] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:24:09,510] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1595472, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1595472, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 12:24:09,510] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 12:24:09,510] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:24:09,618] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 12:24:16,125] INFO [connector1|task-0] 3 records sent during previous 00:00:06.829, last recorded offset: {transaction_id=null, ts_sec=1660373655, file=SF-CPU-562-bin.000076, pos=1596604, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 12:24:16,129] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:24:19,305] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:24:29,307] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:24:39,312] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:24:49,318] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:24:59,327] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:25:09,329] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:25:19,340] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:25:29,351] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:25:39,365] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:25:49,374] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:25:59,381] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:26:09,394] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:26:19,404] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:26:29,418] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:26:39,427] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:26:49,451] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:26:59,459] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:27:09,468] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:27:19,479] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:27:29,484] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:27:39,485] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:27:49,486] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:27:59,498] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:28:09,512] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:28:19,525] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:28:29,532] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:28:39,542] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:28:49,556] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:28:59,572] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:29:09,577] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:29:14,920] ERROR Uncaught exception in REST call to /connectors/connector1/config (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot deserialize value of type `java.lang.String` from Array value (token `JsonToken.START_ARRAY`)
 at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 23, column: 44] (through reference chain: java.util.LinkedHashMap["transforms.unwrap.add.source.fields"])
	at com.fasterxml.jackson.databind.exc.MismatchedInputException.from(MismatchedInputException.java:59)
	at com.fasterxml.jackson.databind.DeserializationContext.reportInputMismatch(DeserializationContext.java:1601)
	at com.fasterxml.jackson.databind.DeserializationContext.handleUnexpectedToken(DeserializationContext.java:1375)
	at com.fasterxml.jackson.databind.deser.std.StdDeserializer._deserializeFromArray(StdDeserializer.java:222)
	at com.fasterxml.jackson.databind.deser.std.StringDeserializer.deserialize(StringDeserializer.java:46)
	at com.fasterxml.jackson.databind.deser.std.StringDeserializer.deserialize(StringDeserializer.java:11)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:609)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:437)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:32)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:322)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:2007)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1174)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:816)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:885)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:290)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:475)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:397)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:255)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:234)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:380)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:386)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:833)
[2022-08-13 12:29:19,586] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:29:29,596] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:29:39,376] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 12:29:39,378] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:29:39,379] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 12:29:39,379] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 12:29:39,379] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 12:29:39,380] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 12:29:39,380] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 12:29:39,380] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:29:39,381] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:29:39,381] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 12:29:39,381] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 12:29:39,382] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:29:39,383] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:29:39,383] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 12:29:39,777] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:29:39,778] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 12:29:39,794] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 12:29:39,794] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660373655, file=SF-CPU-562-bin.000076, pos=1596901, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 12:29:39,799] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:29:39,800] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:29:39,804] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:29:39,805] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:29:39,805] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:29:39,806] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:29:39,806] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:29:39,809] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:29:39,812] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:29:39,813] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:29:39,813] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:29:39,814] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 12:29:39,815] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 12:29:39,816] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:29:39,816] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 12:29:39,816] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 12:29:39,817] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:29:39,818] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 12:29:39,818] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 12:29:39,818] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 12:29:39,819] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 12:29:39,819] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 12:29:39,820] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 12:29:39,821] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 12:29:39,821] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:29:39,827] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 12:29:39,831] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:29:39,831] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:29:39,831] INFO [connector1|task-0] Kafka startTimeMs: 1660373979831 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:29:39,833] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 12:29:39,833] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,834] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,834] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,835] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,835] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,835] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,836] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,836] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,836] INFO [connector1|task-0]    transformation = or conversion errors (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,836] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:29:39,836] INFO [connector1|task-0]    transforms.unwrap.add.source.fields = table (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,836] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,837] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,837] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,837] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,837] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,837] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,837] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,837] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,838] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,838] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,838] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,838] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,838] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 12:29:39,849] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1596212, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 12:29:39,855] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 12:29:39,857] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 12:29:39,857] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 12:29:39,857] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 12:29:39,861] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:29:39,861] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:29:39,861] INFO [connector1|task-0] Kafka startTimeMs: 1660373979861 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:29:39,861] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 12:29:39,862] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:29:39,863] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:29:39,863] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:29:39,866] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:29:39,866] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:29:39,866] INFO [connector1|task-0] Kafka startTimeMs: 1660373979866 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:29:39,870] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:29:39,872] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:29:39,879] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:29:39,879] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:29:39,879] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:29:39,879] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:29:39,880] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:29:39,881] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:29:39,885] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:29:39,885] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:29:39,885] INFO [connector1|task-0] Kafka startTimeMs: 1660373979885 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:29:39,885] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 12:29:39,886] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 12:29:39,890] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:29:39,890] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:29:39,894] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:29:39,894] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:29:39,895] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:29:39,895] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:29:39,895] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:29:39,895] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:29:39,895] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 12:29:39,895] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:29:39,895] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:29:39,896] INFO [connector1|task-0] Kafka startTimeMs: 1660373979895 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:29:39,897] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:29:39,898] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:29:39,898] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:29:39,898] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:29:39,898] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:29:39,899] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:29:39,901] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:29:39,910] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 12:29:39,911] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:29:39,912] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:29:39,912] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:29:39,912] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:29:39,914] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:29:39,914] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:29:39,914] INFO [connector1|task-0] Kafka startTimeMs: 1660373979913 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:29:39,917] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:29:39,919] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:29:39,919] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:29:39,920] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:29:39,920] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:29:39,920] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:29:39,921] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:29:39,922] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:29:39,924] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:29:39,925] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:29:39,925] INFO [connector1|task-0] Kafka startTimeMs: 1660373979924 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:29:39,928] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:29:39,928] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:29:39,935] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:29:39,940] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:29:39,940] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:29:39,940] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:29:39,940] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:29:39,941] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:29:39,941] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 12:29:39,942] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 12:29:39,947] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 12:29:39,947] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 12:29:39,947] INFO [connector1|task-0] Kafka startTimeMs: 1660373979947 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 12:29:39,947] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 12:29:39,954] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:29:39,961] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 12:29:39,965] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 12:29:39,966] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:29:39,972] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:29:39,973] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 12:29:39,975] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=1, memberId='order_server-dbhistory-9955b7cd-dac1-4fc1-9591-b6a325a05b7a', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 12:29:39,975] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 1: {order_server-dbhistory-9955b7cd-dac1-4fc1-9591-b6a325a05b7a=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 12:29:39,978] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=1, memberId='order_server-dbhistory-9955b7cd-dac1-4fc1-9591-b6a325a05b7a', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 12:29:39,978] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 12:29:39,978] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 12:29:39,980] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 12:29:39,985] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 12:29:40,012] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 12:29:40,012] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-9955b7cd-dac1-4fc1-9591-b6a325a05b7a sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 12:29:40,014] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 12:29:40,014] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 12:29:40,018] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:29:40,018] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:29:40,019] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:29:40,020] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:29:40,020] INFO [connector1|task-0] Finished database history recovery of 15 change(s) in 78 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 12:29:40,020] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 12:29:40,024] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 12:29:40,028] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 12:29:40,030] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 12:29:40,030] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 12:29:40,031] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 12:29:40,031] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 12:29:40,031] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 12:29:40,031] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 12:29:40,031] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 12:29:40,031] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1596212, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1596212, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 12:29:40,032] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 12:29:40,032] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 12:29:40,035] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 12:29:40,036] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 12:29:40,036] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:29:40,039] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:29:40,050] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1596212, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1596212, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 12:29:40,050] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 12:29:40,050] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 12:29:40,151] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 12:29:48,725] INFO [connector1|task-0] 3 records sent during previous 00:00:08.908, last recorded offset: {transaction_id=null, ts_sec=1660373988, file=SF-CPU-562-bin.000076, pos=1597736, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 12:29:48,729] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 12:29:49,845] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:29:59,865] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:30:09,868] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:30:19,879] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:30:29,882] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:30:39,888] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:30:49,903] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:30:59,917] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:31:09,925] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:31:19,933] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:31:29,936] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:31:39,942] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:31:46,029] INFO [connector1|task-0] 3 records sent during previous 00:01:57.304, last recorded offset: {transaction_id=null, ts_sec=1660374105, file=SF-CPU-562-bin.000076, pos=1598868, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 12:31:49,945] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:31:59,963] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:32:09,977] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:32:19,983] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:32:29,996] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:32:40,005] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:32:50,010] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:33:00,022] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:33:10,036] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:33:20,051] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:33:21,480] INFO [connector1|task-0] 3 records sent during previous 00:01:35.451, last recorded offset: {transaction_id=null, ts_sec=1660374201, file=SF-CPU-562-bin.000076, pos=1600000, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 12:33:30,056] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:33:40,064] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:33:50,069] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:34:00,073] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:34:10,081] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:34:20,089] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:34:30,102] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:34:40,114] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:34:50,126] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:35:00,138] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:35:10,145] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:35:20,161] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:35:30,163] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:35:40,171] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:35:50,183] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:36:00,197] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:36:10,211] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:36:20,213] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:36:30,222] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:36:40,232] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:36:50,238] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:37:00,254] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:37:10,256] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:37:20,266] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:37:30,273] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:37:40,275] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:37:50,282] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:38:00,292] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:38:10,303] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:38:20,310] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:38:30,315] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:38:39,926] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:38:39,942] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 12:38:40,332] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:38:50,333] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:39:00,349] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:39:10,360] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:39:20,368] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:39:30,371] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:39:40,374] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:39:50,389] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:40:00,398] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:40:10,402] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:40:20,411] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:40:30,415] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:40:40,422] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:40:50,434] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:40:58,791] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2022-08-13 12:40:58,792] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:311)
[2022-08-13 12:40:58,803] INFO Stopped http_8083@66434cc8{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2022-08-13 12:40:58,804] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2022-08-13 12:40:58,806] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:328)
[2022-08-13 12:40:58,806] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:106)
[2022-08-13 12:40:58,807] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 12:40:59,018] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 12:40:59,018] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 12:40:59,048] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 12:40:59,048] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660374201, file=SF-CPU-562-bin.000076, pos=1600297, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 12:40:59,050] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 12:40:59,051] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:40:59,052] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:40:59,052] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:40:59,052] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:40:59,053] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:40:59,053] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 12:40:59,054] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:40:59,054] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:40:59,055] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:40:59,055] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:40:59,055] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 12:40:59,056] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 12:40:59,056] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 12:40:59,056] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:199)
[2022-08-13 12:40:59,057] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2022-08-13 12:40:59,057] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 12:40:59,057] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 12:40:59,057] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 12:40:59,057] INFO App info kafka.connect for 192.168.0.111:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 12:40:59,058] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:220)
[2022-08-13 12:40:59,058] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:124)
[2022-08-13 12:40:59,058] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
