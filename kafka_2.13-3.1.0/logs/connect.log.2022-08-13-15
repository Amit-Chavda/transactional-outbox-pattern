[2022-08-13 15:06:41,613] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 15:06:41,615] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 15:06:45,403] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:06:55,409] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:07:05,410] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:07:15,422] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:07:16,834] INFO [connector1|task-0] 3 records sent during previous 01:18:20.953, last recorded offset: {transaction_id=null, ts_sec=1660383436, file=SF-CPU-562-bin.000076, pos=1610178, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:07:16,836] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:07:25,426] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:07:35,436] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:07:45,441] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:07:55,442] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:08:05,443] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:08:15,451] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:08:25,454] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:08:35,467] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:08:45,474] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:08:55,487] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:09:05,498] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:09:15,501] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:09:25,503] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:09:35,507] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:09:45,517] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:09:55,074] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 15:09:55,076] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:09:55,076] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 15:09:55,077] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 15:09:55,077] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 15:09:55,077] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 15:09:55,078] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 15:09:55,078] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:09:55,078] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:09:55,079] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 15:09:55,079] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 15:09:55,080] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:09:55,080] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:09:55,081] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 15:09:55,231] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:09:55,232] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 15:09:55,249] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 15:09:55,250] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660383491, file=SF-CPU-562-bin.000076, pos=1611724, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 15:09:55,254] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:09:55,256] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:09:55,264] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:09:55,267] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:09:55,267] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:09:55,268] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:09:55,268] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:09:55,273] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:09:55,274] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:09:55,274] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:09:55,275] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:09:55,279] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 15:09:55,282] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 15:09:55,283] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:09:55,283] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 15:09:55,284] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 15:09:55,284] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:09:55,285] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:09:55,285] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 15:09:55,285] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 15:09:55,285] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 15:09:55,286] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:09:55,287] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:09:55,288] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 15:09:55,288] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:09:55,292] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 15:09:55,293] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:09:55,293] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:09:55,293] INFO [connector1|task-0] Kafka startTimeMs: 1660383595293 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:09:55,295] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 15:09:55,296] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:09:55,300] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,301] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,301] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,301] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,301] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,302] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,302] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,302] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,302] INFO [connector1|task-0]    transformation = or conversion errors (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,302] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,302] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,302] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,302] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,302] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,303] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,303] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,303] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,303] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,303] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,303] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,303] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,303] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:09:55,313] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1610957, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 15:09:55,320] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 15:09:55,320] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 15:09:55,321] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 15:09:55,322] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:09:55,326] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:09:55,331] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:09:55,332] INFO [connector1|task-0] Kafka startTimeMs: 1660383595326 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:09:55,332] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 15:09:55,333] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:09:55,333] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:09:55,334] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:09:55,338] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:09:55,339] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:09:55,340] INFO [connector1|task-0] Kafka startTimeMs: 1660383595338 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:09:55,343] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:09:55,349] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:09:55,349] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:09:55,350] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:09:55,350] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:09:55,350] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:09:55,351] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:09:55,351] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:09:55,356] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:09:55,362] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:09:55,362] INFO [connector1|task-0] Kafka startTimeMs: 1660383595356 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:09:55,363] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 15:09:55,363] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 15:09:55,366] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:09:55,367] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:09:55,367] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:09:55,367] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:09:55,367] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:09:55,367] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:09:55,367] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:09:55,368] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:09:55,368] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:09:55,368] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:09:55,368] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:09:55,368] INFO [connector1|task-0] Kafka startTimeMs: 1660383595368 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:09:55,371] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:09:55,372] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:09:55,379] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:09:55,379] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:09:55,380] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:09:55,380] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 15:09:55,380] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:09:55,381] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:09:55,382] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:09:55,382] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:09:55,382] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:09:55,382] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:09:55,391] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:09:55,391] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:09:55,391] INFO [connector1|task-0] Kafka startTimeMs: 1660383595390 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:09:55,394] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:09:55,395] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:09:55,396] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:09:55,396] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:09:55,396] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:09:55,396] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:09:55,397] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:09:55,398] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:09:55,402] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:09:55,402] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:09:55,402] INFO [connector1|task-0] Kafka startTimeMs: 1660383595402 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:09:55,409] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:09:55,411] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:09:55,423] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:09:55,423] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:09:55,424] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:09:55,424] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:09:55,424] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:09:55,425] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:09:55,426] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 15:09:55,427] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:09:55,432] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:09:55,432] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:09:55,432] INFO [connector1|task-0] Kafka startTimeMs: 1660383595432 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:09:55,433] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 15:09:55,440] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:09:55,446] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:09:55,450] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 15:09:55,451] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:09:55,455] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:09:55,455] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:09:55,459] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=1, memberId='order_server-dbhistory-a6f15092-5d71-4e84-9067-f09d0bf6d101', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 15:09:55,459] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 1: {order_server-dbhistory-a6f15092-5d71-4e84-9067-f09d0bf6d101=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 15:09:55,461] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=1, memberId='order_server-dbhistory-a6f15092-5d71-4e84-9067-f09d0bf6d101', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 15:09:55,461] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 15:09:55,461] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 15:09:55,462] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 15:09:55,464] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 15:09:55,506] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 15:09:55,507] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-a6f15092-5d71-4e84-9067-f09d0bf6d101 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 15:09:55,509] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:09:55,509] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:09:55,513] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:09:55,513] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:09:55,513] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:09:55,514] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:09:55,515] INFO [connector1|task-0] Finished database history recovery of 27 change(s) in 89 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 15:09:55,515] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 15:09:55,519] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 15:09:55,521] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 15:09:55,522] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 15:09:55,522] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 15:09:55,522] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 15:09:55,523] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 15:09:55,523] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 15:09:55,523] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 15:09:55,524] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 15:09:55,524] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1610957, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1610957, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 15:09:55,524] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 15:09:55,524] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 15:09:55,530] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 15:09:55,543] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 15:09:55,544] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:09:55,545] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:09:55,548] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1610957, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1610957, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 15:09:55,549] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 15:09:55,549] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:09:55,653] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 15:10:04,719] INFO [connector1|task-0] 3 records sent during previous 00:00:09.434, last recorded offset: {transaction_id=null, ts_sec=1660383604, file=SF-CPU-562-bin.000076, pos=1612598, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:10:04,723] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:10:05,308] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:10:15,321] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:10:24,670] INFO [connector1|task-0] 3 records sent during previous 00:00:19.951, last recorded offset: {transaction_id=null, ts_sec=1660383624, file=SF-CPU-562-bin.000076, pos=1613808, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:10:25,324] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:10:35,340] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:10:45,352] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:10:55,354] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:11:05,361] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:11:15,364] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:11:20,869] INFO [connector1|task-0] 3 records sent during previous 00:00:56.199, last recorded offset: {transaction_id=null, ts_sec=1660383680, file=SF-CPU-562-bin.000076, pos=1615018, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:11:25,368] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:11:35,383] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:11:45,386] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:11:55,392] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:12:05,408] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:12:15,415] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:12:25,428] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:12:35,430] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:12:45,438] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:12:55,439] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:13:05,447] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:13:15,452] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:13:25,462] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:13:35,473] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:13:36,785] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 15:13:36,786] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:13:36,787] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 15:13:36,787] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 15:13:36,787] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 15:13:36,787] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 15:13:36,788] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 15:13:36,788] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:13:36,788] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:13:36,789] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 15:13:36,789] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 15:13:36,789] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:13:36,790] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:13:36,791] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 15:13:36,854] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:13:36,855] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 15:13:36,901] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 15:13:36,901] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660383680, file=SF-CPU-562-bin.000076, pos=1615354, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 15:13:36,904] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:13:36,904] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:13:36,906] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:13:36,906] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:13:36,907] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:13:36,907] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:13:36,908] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:13:36,910] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:13:36,911] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:13:36,911] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:13:36,911] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:13:36,912] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 15:13:36,913] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 15:13:36,914] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:13:36,915] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 15:13:36,915] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 15:13:36,915] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:13:36,916] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:13:36,916] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 15:13:36,916] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 15:13:36,923] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 15:13:36,923] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:13:36,924] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:13:36,925] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 15:13:36,925] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:13:36,929] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 15:13:36,929] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:13:36,929] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:13:36,929] INFO [connector1|task-0] Kafka startTimeMs: 1660383816929 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:13:36,930] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 15:13:36,930] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,931] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,931] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,931] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,931] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,931] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,931] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,931] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,931] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:13:36,932] INFO [connector1|task-0]    transformation = or conversion errors (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,932] INFO [connector1|task-0]    transforms.unwrap.add.source.fields = table (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,932] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,932] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,932] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,932] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,932] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,932] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,933] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,938] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,938] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,938] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,938] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,939] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,939] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:13:36,948] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1614587, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 15:13:36,957] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 15:13:36,957] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 15:13:36,957] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 15:13:36,958] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:13:36,961] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:13:36,962] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:13:36,962] INFO [connector1|task-0] Kafka startTimeMs: 1660383816961 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:13:36,962] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 15:13:36,962] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:13:36,965] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:13:36,969] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:13:36,977] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:13:36,977] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:13:36,977] INFO [connector1|task-0] Kafka startTimeMs: 1660383816977 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:13:36,980] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:13:36,985] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:13:36,985] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:13:36,986] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:13:36,986] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:13:36,986] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:13:36,987] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:13:36,987] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:13:36,990] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:13:36,991] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:13:36,991] INFO [connector1|task-0] Kafka startTimeMs: 1660383816990 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:13:36,991] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 15:13:36,993] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 15:13:36,995] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:13:36,997] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:13:37,002] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:13:37,002] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:13:37,002] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:13:37,002] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:13:37,002] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:13:37,002] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:13:37,002] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:13:37,003] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:13:37,003] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:13:37,003] INFO [connector1|task-0] Kafka startTimeMs: 1660383817003 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:13:37,007] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:13:37,007] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:13:37,008] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:13:37,008] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:13:37,008] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:13:37,010] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:13:37,015] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:13:37,016] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 15:13:37,016] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:13:37,018] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:13:37,018] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:13:37,018] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:13:37,020] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:13:37,020] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:13:37,020] INFO [connector1|task-0] Kafka startTimeMs: 1660383817020 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:13:37,023] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:13:37,025] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:13:37,033] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:13:37,034] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:13:37,034] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:13:37,034] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:13:37,035] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:13:37,035] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:13:37,039] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:13:37,039] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:13:37,039] INFO [connector1|task-0] Kafka startTimeMs: 1660383817039 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:13:37,043] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:13:37,044] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:13:37,049] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:13:37,049] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:13:37,049] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:13:37,049] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:13:37,050] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:13:37,051] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:13:37,051] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 15:13:37,051] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:13:37,055] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:13:37,055] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:13:37,055] INFO [connector1|task-0] Kafka startTimeMs: 1660383817055 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:13:37,055] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 15:13:37,060] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:13:37,065] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:13:37,069] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 15:13:37,070] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:13:37,072] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:13:37,073] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:13:37,075] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=1, memberId='order_server-dbhistory-10f9f198-ba9b-4998-974b-f24ab70c9f82', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 15:13:37,076] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 1: {order_server-dbhistory-10f9f198-ba9b-4998-974b-f24ab70c9f82=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 15:13:37,079] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=1, memberId='order_server-dbhistory-10f9f198-ba9b-4998-974b-f24ab70c9f82', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 15:13:37,079] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 15:13:37,079] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 15:13:37,081] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 15:13:37,085] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 15:13:37,118] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 15:13:37,119] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-10f9f198-ba9b-4998-974b-f24ab70c9f82 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 15:13:37,119] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:13:37,119] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:13:37,127] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:13:37,127] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:13:37,128] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:13:37,129] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:13:37,129] INFO [connector1|task-0] Finished database history recovery of 27 change(s) in 77 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 15:13:37,138] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 15:13:37,144] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 15:13:37,146] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 15:13:37,147] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 15:13:37,148] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 15:13:37,148] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 15:13:37,149] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 15:13:37,163] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 15:13:37,169] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 15:13:37,170] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 15:13:37,170] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1614587, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1614587, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 15:13:37,170] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 15:13:37,170] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 15:13:37,174] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 15:13:37,174] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 15:13:37,175] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:13:37,177] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:13:37,184] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1614587, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1614587, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 15:13:37,185] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 15:13:37,185] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:13:37,290] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 15:13:44,271] INFO [connector1|task-0] 3 records sent during previous 00:00:07.355, last recorded offset: {transaction_id=null, ts_sec=1660383823, file=SF-CPU-562-bin.000076, pos=1616228, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:13:44,280] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:13:46,934] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:13:56,940] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:14:06,954] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:14:16,965] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:14:26,977] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:14:36,983] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:14:46,991] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:14:57,003] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:15:07,010] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:15:17,013] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:15:27,017] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:15:37,025] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:15:47,028] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:15:57,045] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:16:07,055] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:16:17,068] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:16:27,073] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:16:37,088] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:16:47,104] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:16:57,113] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:17:07,125] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:17:14,743] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2022-08-13 15:17:14,743] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:311)
[2022-08-13 15:17:14,746] INFO Stopped http_8083@66434cc8{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2022-08-13 15:17:14,747] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2022-08-13 15:17:14,747] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:328)
[2022-08-13 15:17:14,747] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:106)
[2022-08-13 15:17:14,747] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 15:17:14,898] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:17:14,899] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 15:17:14,991] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 15:17:14,991] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660383823, file=SF-CPU-562-bin.000076, pos=1616564, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 15:17:14,996] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:17:14,997] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:17:15,001] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:17:15,002] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:17:15,002] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:17:15,003] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:17:15,004] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:17:15,007] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:17:15,009] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:17:15,009] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:17:15,009] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:17:15,010] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 15:17:15,010] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 15:17:15,010] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 15:17:15,010] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:199)
[2022-08-13 15:17:15,011] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2022-08-13 15:17:15,011] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:17:15,011] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:17:15,011] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:17:15,012] INFO App info kafka.connect for 192.168.0.111:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:17:15,012] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:220)
[2022-08-13 15:17:15,012] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:124)
[2022-08-13 15:17:15,013] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2022-08-13 15:17:18,138] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:68)
[2022-08-13 15:17:18,146] INFO WorkerInfo values: 
	jvm.args = -Xmx256M, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=C:\kafka/logs, -Dlog4j.configuration=file:C:\kafka/config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 17.0.2, 17.0.2+8-LTS-86
	jvm.classpath = C:\kafka\libs\activation-1.1.1.jar;C:\kafka\libs\aopalliance-repackaged-2.6.1.jar;C:\kafka\libs\argparse4j-0.7.0.jar;C:\kafka\libs\audience-annotations-0.5.0.jar;C:\kafka\libs\commons-cli-1.4.jar;C:\kafka\libs\commons-lang3-3.8.1.jar;C:\kafka\libs\connect-api-3.1.0.jar;C:\kafka\libs\connect-basic-auth-extension-3.1.0.jar;C:\kafka\libs\connect-file-3.1.0.jar;C:\kafka\libs\connect-json-3.1.0.jar;C:\kafka\libs\connect-mirror-3.1.0.jar;C:\kafka\libs\connect-mirror-client-3.1.0.jar;C:\kafka\libs\connect-runtime-3.1.0.jar;C:\kafka\libs\connect-transforms-3.1.0.jar;C:\kafka\libs\hk2-api-2.6.1.jar;C:\kafka\libs\hk2-locator-2.6.1.jar;C:\kafka\libs\hk2-utils-2.6.1.jar;C:\kafka\libs\jackson-annotations-2.12.3.jar;C:\kafka\libs\jackson-core-2.12.3.jar;C:\kafka\libs\jackson-databind-2.12.3.jar;C:\kafka\libs\jackson-dataformat-csv-2.12.3.jar;C:\kafka\libs\jackson-datatype-jdk8-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-base-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-json-provider-2.12.3.jar;C:\kafka\libs\jackson-module-jaxb-annotations-2.12.3.jar;C:\kafka\libs\jackson-module-scala_2.13-2.12.3.jar;C:\kafka\libs\jakarta.activation-api-1.2.1.jar;C:\kafka\libs\jakarta.annotation-api-1.3.5.jar;C:\kafka\libs\jakarta.inject-2.6.1.jar;C:\kafka\libs\jakarta.validation-api-2.0.2.jar;C:\kafka\libs\jakarta.ws.rs-api-2.1.6.jar;C:\kafka\libs\jakarta.xml.bind-api-2.3.2.jar;C:\kafka\libs\javassist-3.27.0-GA.jar;C:\kafka\libs\javax.servlet-api-3.1.0.jar;C:\kafka\libs\javax.ws.rs-api-2.1.1.jar;C:\kafka\libs\jaxb-api-2.3.0.jar;C:\kafka\libs\jersey-client-2.34.jar;C:\kafka\libs\jersey-common-2.34.jar;C:\kafka\libs\jersey-container-servlet-2.34.jar;C:\kafka\libs\jersey-container-servlet-core-2.34.jar;C:\kafka\libs\jersey-hk2-2.34.jar;C:\kafka\libs\jersey-server-2.34.jar;C:\kafka\libs\jetty-client-9.4.43.v20210629.jar;C:\kafka\libs\jetty-continuation-9.4.43.v20210629.jar;C:\kafka\libs\jetty-http-9.4.43.v20210629.jar;C:\kafka\libs\jetty-io-9.4.43.v20210629.jar;C:\kafka\libs\jetty-security-9.4.43.v20210629.jar;C:\kafka\libs\jetty-server-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlet-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlets-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-ajax-9.4.43.v20210629.jar;C:\kafka\libs\jline-3.12.1.jar;C:\kafka\libs\jopt-simple-5.0.4.jar;C:\kafka\libs\jose4j-0.7.8.jar;C:\kafka\libs\kafka-clients-3.1.0.jar;C:\kafka\libs\kafka-log4j-appender-3.1.0.jar;C:\kafka\libs\kafka-metadata-3.1.0.jar;C:\kafka\libs\kafka-raft-3.1.0.jar;C:\kafka\libs\kafka-server-common-3.1.0.jar;C:\kafka\libs\kafka-shell-3.1.0.jar;C:\kafka\libs\kafka-storage-3.1.0.jar;C:\kafka\libs\kafka-storage-api-3.1.0.jar;C:\kafka\libs\kafka-streams-3.1.0.jar;C:\kafka\libs\kafka-streams-examples-3.1.0.jar;C:\kafka\libs\kafka-streams-scala_2.13-3.1.0.jar;C:\kafka\libs\kafka-streams-test-utils-3.1.0.jar;C:\kafka\libs\kafka-tools-3.1.0.jar;C:\kafka\libs\kafka_2.13-3.1.0.jar;C:\kafka\libs\log4j-1.2.17.jar;C:\kafka\libs\lz4-java-1.8.0.jar;C:\kafka\libs\maven-artifact-3.8.1.jar;C:\kafka\libs\metrics-core-2.2.0.jar;C:\kafka\libs\metrics-core-4.1.12.1.jar;C:\kafka\libs\netty-buffer-4.1.68.Final.jar;C:\kafka\libs\netty-codec-4.1.68.Final.jar;C:\kafka\libs\netty-common-4.1.68.Final.jar;C:\kafka\libs\netty-handler-4.1.68.Final.jar;C:\kafka\libs\netty-resolver-4.1.68.Final.jar;C:\kafka\libs\netty-transport-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-epoll-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-unix-common-4.1.68.Final.jar;C:\kafka\libs\osgi-resource-locator-1.0.3.jar;C:\kafka\libs\paranamer-2.8.jar;C:\kafka\libs\plexus-utils-3.2.1.jar;C:\kafka\libs\reflections-0.9.12.jar;C:\kafka\libs\rocksdbjni-6.22.1.1.jar;C:\kafka\libs\scala-collection-compat_2.13-2.4.4.jar;C:\kafka\libs\scala-java8-compat_2.13-1.0.0.jar;C:\kafka\libs\scala-library-2.13.6.jar;C:\kafka\libs\scala-logging_2.13-3.9.3.jar;C:\kafka\libs\scala-reflect-2.13.6.jar;C:\kafka\libs\slf4j-api-1.7.30.jar;C:\kafka\libs\slf4j-log4j12-1.7.30.jar;C:\kafka\libs\snappy-java-1.1.8.4.jar;C:\kafka\libs\trogdor-3.1.0.jar;C:\kafka\libs\zookeeper-3.6.3.jar;C:\kafka\libs\zookeeper-jute-3.6.3.jar;C:\kafka\libs\zstd-jni-1.5.0-4.jar
	os.spec = Windows 10, amd64, 10.0
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2022-08-13 15:17:18,148] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:77)
[2022-08-13 15:17:18,158] INFO Loading plugin from: C:\kafka\opt\connectors\debezium-debezium-connector-mysql-1.9.3 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:265)
[2022-08-13 15:17:18,545] INFO Registered loader: PluginClassLoader{pluginLocation=file:/C:/kafka/opt/connectors/debezium-debezium-connector-mysql-1.9.3/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 15:17:18,546] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,547] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,548] INFO Added plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,548] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,548] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,548] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,548] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,548] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,548] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,549] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,549] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,550] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:18,550] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,293] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@266474c2 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 15:17:19,293] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,294] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,295] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,295] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,295] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,295] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,295] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,295] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,295] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,296] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,296] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,296] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,296] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,296] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,296] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,296] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,297] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,297] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,297] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,297] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,297] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,297] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,298] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,298] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,298] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,298] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,298] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,298] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,299] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,299] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,299] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,299] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,299] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,299] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,299] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,300] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,300] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,300] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,300] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,300] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,300] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,300] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,300] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,300] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,301] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,301] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,301] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,301] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,301] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,302] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,302] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,302] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,302] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:17:19,303] INFO Added aliases 'MySqlConnector' and 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,303] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,303] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,303] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,303] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,303] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,304] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,304] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,304] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,304] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,304] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,304] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,305] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,305] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,305] INFO Added aliases 'CloudEventsConverter' and 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,305] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,305] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,305] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,305] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,306] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,306] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,306] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,306] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,306] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,306] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,306] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,306] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,306] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,307] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,307] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,307] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,307] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,307] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,307] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,307] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,308] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,308] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,308] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,308] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,308] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,308] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,308] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,309] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,309] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,309] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,309] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,309] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,309] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:17:19,309] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,309] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,309] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:17:19,325] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = All
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = C:/kafka/tmp/connect.offsets
	plugin.path = [/opt/connectors, C:/kafka/opt/connectors]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:376)
[2022-08-13 15:17:19,326] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 15:17:19,334] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 15:17:19,407] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,407] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,408] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,408] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,409] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,409] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,409] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,409] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:17:19,409] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:17:19,409] INFO Kafka startTimeMs: 1660384039409 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:17:19,638] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 15:17:19,639] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:17:19,645] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:17:19,645] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:17:19,645] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:17:19,654] INFO Logging initialized @1861ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2022-08-13 15:17:19,682] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:117)
[2022-08-13 15:17:19,683] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:188)
[2022-08-13 15:17:19,690] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 17.0.2+8-LTS-86 (org.eclipse.jetty.server.Server:375)
[2022-08-13 15:17:19,709] INFO Started http_8083@66434cc8{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2022-08-13 15:17:19,709] INFO Started @1916ms (org.eclipse.jetty.server.Server:415)
[2022-08-13 15:17:19,726] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 15:17:19,726] INFO REST server listening at http://192.168.0.111:8083/, advertising URL http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2022-08-13 15:17:19,726] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 15:17:19,726] INFO REST admin endpoints at http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2022-08-13 15:17:19,726] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 15:17:19,727] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:44)
[2022-08-13 15:17:19,733] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 15:17:19,733] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 15:17:19,739] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,739] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,739] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,739] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,739] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,739] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,740] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:19,740] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:17:19,740] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:17:19,740] INFO Kafka startTimeMs: 1660384039740 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:17:19,749] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 15:17:19,754] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:17:19,756] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:17:19,757] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:17:19,757] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:17:19,761] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:17:19,761] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:17:19,761] INFO Kafka startTimeMs: 1660384039761 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:17:19,855] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:17:19,857] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:17:19,862] INFO Kafka Connect standalone worker initialization took 1722ms (org.apache.kafka.connect.cli.ConnectStandalone:99)
[2022-08-13 15:17:19,862] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2022-08-13 15:17:19,863] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:98)
[2022-08-13 15:17:19,863] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:185)
[2022-08-13 15:17:19,864] INFO Starting FileOffsetBackingStore with file C:\kafka\tmp\connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2022-08-13 15:17:19,872] INFO Worker started (org.apache.kafka.connect.runtime.Worker:192)
[2022-08-13 15:17:19,873] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:101)
[2022-08-13 15:17:19,873] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:208)
[2022-08-13 15:17:19,898] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:225)
[2022-08-13 15:17:19,940] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2022-08-13 15:17:19,940] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2022-08-13 15:17:19,942] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2022-08-13 15:17:20,271] INFO Started o.e.j.s.ServletContextHandler@221dad51{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:915)
[2022-08-13 15:17:20,271] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2022-08-13 15:17:20,271] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2022-08-13 15:17:20,453] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 15:17:20,459] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:17:20,461] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 15:17:20,472] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 15:17:20,472] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:17:20,473] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:17:20,477] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 15:17:20,477] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 15:17:20,479] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:17:20,480] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:17:20,489] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 15:17:20,491] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 15:17:20,491] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:17:20,493] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 15:17:20,494] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 15:17:20,494] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:17:20,495] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:17:20,495] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 15:17:20,495] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 15:17:20,496] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 15:17:20,499] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:17:20,500] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:17:20,503] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 15:17:20,506] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:17:20,527] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 15:17:20,527] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:17:20,529] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:17:20,529] INFO [connector1|task-0] Kafka startTimeMs: 1660384040527 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:17:20,537] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:17:20,540] INFO Created connector connector1 (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2022-08-13 15:17:20,541] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 15:17:20,542] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,544] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,544] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,544] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,544] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,544] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,544] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,544] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,544] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,545] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,545] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,545] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,545] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,545] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,545] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,546] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,546] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,546] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,546] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,546] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,546] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:17:20,598] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1615797, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 15:17:20,623] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 15:17:20,623] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 15:17:20,624] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 15:17:20,625] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:17:20,630] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:17:20,630] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:17:20,631] INFO [connector1|task-0] Kafka startTimeMs: 1660384040630 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:17:20,634] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:17:20,650] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 15:17:20,651] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:17:20,657] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:17:20,689] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:17:20,691] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:17:20,692] INFO [connector1|task-0] Kafka startTimeMs: 1660384040689 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:17:20,699] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:17:20,702] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:17:20,703] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:17:20,703] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:17:20,704] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:17:20,704] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:17:20,705] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:17:20,706] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:17:20,710] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:17:20,710] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:17:20,710] INFO [connector1|task-0] Kafka startTimeMs: 1660384040710 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:17:20,710] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 15:17:20,711] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 15:17:20,717] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:20,718] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:20,718] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:20,718] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:20,718] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:20,719] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:20,719] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:17:20,719] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:17:20,719] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:17:20,719] INFO [connector1|task-0] Kafka startTimeMs: 1660384040719 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:17:20,720] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:17:20,720] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:17:20,728] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:17:20,737] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:17:20,737] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:17:20,737] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:17:20,738] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:17:20,738] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 15:17:20,739] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:17:20,739] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:17:20,740] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:17:20,740] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:17:20,740] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:17:20,740] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:17:20,744] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:17:20,745] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:17:20,745] INFO [connector1|task-0] Kafka startTimeMs: 1660384040744 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:17:20,749] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:17:20,750] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:17:20,751] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:17:20,751] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:17:20,751] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:17:20,751] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:17:20,752] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:17:20,752] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:17:20,756] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:17:20,756] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:17:20,756] INFO [connector1|task-0] Kafka startTimeMs: 1660384040756 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:17:20,760] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:17:20,765] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:17:20,769] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:17:20,769] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:17:20,769] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:17:20,770] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:17:20,770] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:17:20,771] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:17:20,771] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 15:17:20,777] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:17:20,782] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:17:20,782] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:17:20,782] INFO [connector1|task-0] Kafka startTimeMs: 1660384040782 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:17:20,783] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 15:17:20,787] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:17:20,787] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:17:20,791] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 15:17:20,796] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:17:20,805] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:17:20,806] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:17:20,809] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=1, memberId='order_server-dbhistory-5cd9d40b-d6a9-445e-8075-411e8702ce6b', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 15:17:20,811] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 1: {order_server-dbhistory-5cd9d40b-d6a9-445e-8075-411e8702ce6b=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 15:17:20,816] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=1, memberId='order_server-dbhistory-5cd9d40b-d6a9-445e-8075-411e8702ce6b', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 15:17:20,817] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 15:17:20,819] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 15:17:20,826] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 15:17:20,830] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 15:17:21,293] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 15:17:21,294] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-5cd9d40b-d6a9-445e-8075-411e8702ce6b sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 15:17:21,296] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:17:21,296] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:17:21,301] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:17:21,301] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:17:21,301] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:17:21,303] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:17:21,303] INFO [connector1|task-0] Finished database history recovery of 27 change(s) in 532 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 15:17:21,315] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 15:17:21,319] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 15:17:21,321] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 15:17:21,346] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 15:17:21,349] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 15:17:21,349] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 15:17:21,350] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 15:17:21,352] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 15:17:21,353] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 15:17:21,361] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 15:17:21,362] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1615797, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1615797, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 15:17:21,366] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 15:17:21,368] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 15:17:21,379] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 15:17:21,379] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 15:17:21,379] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:17:21,382] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:17:21,390] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1615797, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1615797, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 15:17:21,390] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 15:17:21,391] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:17:21,495] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 15:17:28,985] INFO [connector1|task-0] 3 records sent during previous 00:00:08.49, last recorded offset: {transaction_id=null, ts_sec=1660384048, file=SF-CPU-562-bin.000076, pos=1617438, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:17:28,994] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:17:30,548] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:17:40,567] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:17:50,572] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:18:00,584] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:18:10,593] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:18:20,598] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:18:30,600] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:18:40,605] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:18:50,612] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:19:00,624] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:19:10,630] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:19:20,641] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:19:30,645] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:19:40,660] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:19:50,661] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:20:00,675] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:20:10,691] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:20:20,693] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:20:30,705] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:20:40,711] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:20:50,713] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:21:00,719] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:21:10,722] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:21:20,728] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:21:24,075] INFO [connector1|task-0] 3 records sent during previous 00:03:55.09, last recorded offset: {transaction_id=null, ts_sec=1660384283, file=SF-CPU-562-bin.000076, pos=1618649, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:21:30,742] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:21:40,748] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:21:50,759] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:22:00,773] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:22:10,784] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:22:20,796] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:22:30,804] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:22:40,806] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:22:50,817] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:23:00,820] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:23:10,835] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:23:20,848] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:23:30,856] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:23:40,865] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:23:50,879] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:24:00,894] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:24:10,909] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:24:20,924] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:24:23,038] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:24:23,041] INFO [connector1|task-0] Already applied 28 database changes (io.debezium.relational.history.DatabaseHistoryMetrics:132)
[2022-08-13 15:24:23,426] INFO [connector1|task-0] 3 records sent during previous 00:02:59.351, last recorded offset: {transaction_id=null, ts_sec=1660384463, file=SF-CPU-562-bin.000076, pos=1619609, server_id=1} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:24:23,434] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server-0 to 0 since the associated topicId changed from null to LVkchKJTThukBrB7dOsgOA (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:24:30,926] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:24:40,947] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:24:44,940] INFO [connector1|task-0] Already applied 31 database changes (io.debezium.relational.history.DatabaseHistoryMetrics:132)
[2022-08-13 15:24:50,968] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:25:00,984] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:25:10,995] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:25:21,001] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:25:31,012] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:25:41,021] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:25:51,033] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:26:01,042] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:26:11,056] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:26:21,060] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:26:27,670] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 15:26:27,675] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:26:31,065] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:26:41,071] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:26:45,575] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 15:26:51,089] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:27:01,105] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:27:11,114] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:27:21,129] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:27:29,381] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2022-08-13 15:27:29,381] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:311)
[2022-08-13 15:27:29,385] INFO Stopped http_8083@66434cc8{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:381)
[2022-08-13 15:27:29,385] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2022-08-13 15:27:29,386] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:328)
[2022-08-13 15:27:29,386] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:106)
[2022-08-13 15:27:29,387] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 15:27:29,535] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:27:29,536] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 15:27:29,615] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 15:27:29,615] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660384599, file=SF-CPU-562-bin.000076, pos=1622876, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 15:27:29,621] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:27:29,622] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:27:29,628] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:27:29,628] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:27:29,628] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:27:29,631] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:27:29,631] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:27:29,633] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:27:29,633] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:27:29,633] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:27:29,633] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:27:29,635] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 15:27:29,635] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 15:27:29,635] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 15:27:29,635] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:199)
[2022-08-13 15:27:29,636] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:66)
[2022-08-13 15:27:29,636] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:27:29,636] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:27:29,637] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:27:29,637] INFO App info kafka.connect for 192.168.0.111:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:27:29,637] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:220)
[2022-08-13 15:27:29,637] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:124)
[2022-08-13 15:27:29,638] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2022-08-13 15:27:34,095] INFO Kafka Connect standalone worker initializing ... (org.apache.kafka.connect.cli.ConnectStandalone:68)
[2022-08-13 15:27:34,104] INFO WorkerInfo values: 
	jvm.args = -Xmx256M, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=C:\kafka/logs, -Dlog4j.configuration=file:C:\kafka/config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 17.0.2, 17.0.2+8-LTS-86
	jvm.classpath = C:\kafka\libs\activation-1.1.1.jar;C:\kafka\libs\aopalliance-repackaged-2.6.1.jar;C:\kafka\libs\argparse4j-0.7.0.jar;C:\kafka\libs\audience-annotations-0.5.0.jar;C:\kafka\libs\commons-cli-1.4.jar;C:\kafka\libs\commons-lang3-3.8.1.jar;C:\kafka\libs\connect-api-3.1.0.jar;C:\kafka\libs\connect-basic-auth-extension-3.1.0.jar;C:\kafka\libs\connect-file-3.1.0.jar;C:\kafka\libs\connect-json-3.1.0.jar;C:\kafka\libs\connect-mirror-3.1.0.jar;C:\kafka\libs\connect-mirror-client-3.1.0.jar;C:\kafka\libs\connect-runtime-3.1.0.jar;C:\kafka\libs\connect-transforms-3.1.0.jar;C:\kafka\libs\hk2-api-2.6.1.jar;C:\kafka\libs\hk2-locator-2.6.1.jar;C:\kafka\libs\hk2-utils-2.6.1.jar;C:\kafka\libs\jackson-annotations-2.12.3.jar;C:\kafka\libs\jackson-core-2.12.3.jar;C:\kafka\libs\jackson-databind-2.12.3.jar;C:\kafka\libs\jackson-dataformat-csv-2.12.3.jar;C:\kafka\libs\jackson-datatype-jdk8-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-base-2.12.3.jar;C:\kafka\libs\jackson-jaxrs-json-provider-2.12.3.jar;C:\kafka\libs\jackson-module-jaxb-annotations-2.12.3.jar;C:\kafka\libs\jackson-module-scala_2.13-2.12.3.jar;C:\kafka\libs\jakarta.activation-api-1.2.1.jar;C:\kafka\libs\jakarta.annotation-api-1.3.5.jar;C:\kafka\libs\jakarta.inject-2.6.1.jar;C:\kafka\libs\jakarta.validation-api-2.0.2.jar;C:\kafka\libs\jakarta.ws.rs-api-2.1.6.jar;C:\kafka\libs\jakarta.xml.bind-api-2.3.2.jar;C:\kafka\libs\javassist-3.27.0-GA.jar;C:\kafka\libs\javax.servlet-api-3.1.0.jar;C:\kafka\libs\javax.ws.rs-api-2.1.1.jar;C:\kafka\libs\jaxb-api-2.3.0.jar;C:\kafka\libs\jersey-client-2.34.jar;C:\kafka\libs\jersey-common-2.34.jar;C:\kafka\libs\jersey-container-servlet-2.34.jar;C:\kafka\libs\jersey-container-servlet-core-2.34.jar;C:\kafka\libs\jersey-hk2-2.34.jar;C:\kafka\libs\jersey-server-2.34.jar;C:\kafka\libs\jetty-client-9.4.43.v20210629.jar;C:\kafka\libs\jetty-continuation-9.4.43.v20210629.jar;C:\kafka\libs\jetty-http-9.4.43.v20210629.jar;C:\kafka\libs\jetty-io-9.4.43.v20210629.jar;C:\kafka\libs\jetty-security-9.4.43.v20210629.jar;C:\kafka\libs\jetty-server-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlet-9.4.43.v20210629.jar;C:\kafka\libs\jetty-servlets-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-9.4.43.v20210629.jar;C:\kafka\libs\jetty-util-ajax-9.4.43.v20210629.jar;C:\kafka\libs\jline-3.12.1.jar;C:\kafka\libs\jopt-simple-5.0.4.jar;C:\kafka\libs\jose4j-0.7.8.jar;C:\kafka\libs\kafka-clients-3.1.0.jar;C:\kafka\libs\kafka-log4j-appender-3.1.0.jar;C:\kafka\libs\kafka-metadata-3.1.0.jar;C:\kafka\libs\kafka-raft-3.1.0.jar;C:\kafka\libs\kafka-server-common-3.1.0.jar;C:\kafka\libs\kafka-shell-3.1.0.jar;C:\kafka\libs\kafka-storage-3.1.0.jar;C:\kafka\libs\kafka-storage-api-3.1.0.jar;C:\kafka\libs\kafka-streams-3.1.0.jar;C:\kafka\libs\kafka-streams-examples-3.1.0.jar;C:\kafka\libs\kafka-streams-scala_2.13-3.1.0.jar;C:\kafka\libs\kafka-streams-test-utils-3.1.0.jar;C:\kafka\libs\kafka-tools-3.1.0.jar;C:\kafka\libs\kafka_2.13-3.1.0.jar;C:\kafka\libs\log4j-1.2.17.jar;C:\kafka\libs\lz4-java-1.8.0.jar;C:\kafka\libs\maven-artifact-3.8.1.jar;C:\kafka\libs\metrics-core-2.2.0.jar;C:\kafka\libs\metrics-core-4.1.12.1.jar;C:\kafka\libs\netty-buffer-4.1.68.Final.jar;C:\kafka\libs\netty-codec-4.1.68.Final.jar;C:\kafka\libs\netty-common-4.1.68.Final.jar;C:\kafka\libs\netty-handler-4.1.68.Final.jar;C:\kafka\libs\netty-resolver-4.1.68.Final.jar;C:\kafka\libs\netty-transport-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-epoll-4.1.68.Final.jar;C:\kafka\libs\netty-transport-native-unix-common-4.1.68.Final.jar;C:\kafka\libs\osgi-resource-locator-1.0.3.jar;C:\kafka\libs\paranamer-2.8.jar;C:\kafka\libs\plexus-utils-3.2.1.jar;C:\kafka\libs\reflections-0.9.12.jar;C:\kafka\libs\rocksdbjni-6.22.1.1.jar;C:\kafka\libs\scala-collection-compat_2.13-2.4.4.jar;C:\kafka\libs\scala-java8-compat_2.13-1.0.0.jar;C:\kafka\libs\scala-library-2.13.6.jar;C:\kafka\libs\scala-logging_2.13-3.9.3.jar;C:\kafka\libs\scala-reflect-2.13.6.jar;C:\kafka\libs\slf4j-api-1.7.30.jar;C:\kafka\libs\slf4j-log4j12-1.7.30.jar;C:\kafka\libs\snappy-java-1.1.8.4.jar;C:\kafka\libs\trogdor-3.1.0.jar;C:\kafka\libs\zookeeper-3.6.3.jar;C:\kafka\libs\zookeeper-jute-3.6.3.jar;C:\kafka\libs\zstd-jni-1.5.0-4.jar
	os.spec = Windows 10, amd64, 10.0
	os.vcpus = 8
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2022-08-13 15:27:34,106] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectStandalone:77)
[2022-08-13 15:27:34,117] INFO Loading plugin from: C:\kafka\opt\connectors\debezium-debezium-connector-mysql-1.9.3 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:265)
[2022-08-13 15:27:34,510] INFO Registered loader: PluginClassLoader{pluginLocation=file:/C:/kafka/opt/connectors/debezium-debezium-connector-mysql-1.9.3/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 15:27:34,511] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,512] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,512] INFO Added plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,512] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,513] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,513] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,513] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,513] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,513] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,514] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,514] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,515] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:34,515] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,243] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@266474c2 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:288)
[2022-08-13 15:27:35,243] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,244] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,244] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,245] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,245] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,246] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,246] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,246] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,246] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,246] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,247] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,247] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,247] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,247] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,247] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,247] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,248] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,248] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,248] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,248] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,248] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,249] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,249] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,249] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,249] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,249] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,249] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,250] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,250] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,250] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,251] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,251] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,251] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,251] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,251] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,251] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,251] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,251] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,251] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,252] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,252] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,252] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,252] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,252] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,253] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,253] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,253] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,253] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,253] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,253] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,253] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,253] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,254] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:217)
[2022-08-13 15:27:35,254] INFO Added aliases 'MySqlConnector' and 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,254] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,254] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,255] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,255] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,255] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,255] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,255] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,255] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,255] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,255] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,255] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,256] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,256] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,256] INFO Added aliases 'CloudEventsConverter' and 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,256] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,256] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,256] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,256] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,257] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,257] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,257] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,257] INFO Added aliases 'BinaryDataConverter' and 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,257] INFO Added aliases 'ByteBufferConverter' and 'ByteBuffer' to plugin 'io.debezium.converters.ByteBufferConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,257] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,257] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,258] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,258] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,258] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,258] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,258] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,258] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,258] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,259] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,259] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,259] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,259] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,259] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,259] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,259] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,260] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,260] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,260] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,260] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,260] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,260] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,260] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,261] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:459)
[2022-08-13 15:27:35,261] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,261] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,261] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:462)
[2022-08-13 15:27:35,277] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = All
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = C:/kafka/tmp/connect.offsets
	plugin.path = [/opt/connectors, C:/kafka/opt/connectors]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:376)
[2022-08-13 15:27:35,278] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 15:27:35,282] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 15:27:35,358] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,359] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,360] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,360] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,360] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,360] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,360] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,360] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:27:35,361] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:27:35,361] INFO Kafka startTimeMs: 1660384655360 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:27:35,586] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 15:27:35,586] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:27:35,591] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:27:35,591] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:27:35,592] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:27:35,599] INFO Logging initialized @1848ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2022-08-13 15:27:35,626] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:117)
[2022-08-13 15:27:35,627] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:188)
[2022-08-13 15:27:35,632] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 17.0.2+8-LTS-86 (org.eclipse.jetty.server.Server:375)
[2022-08-13 15:27:35,652] INFO Started http_8083@66434cc8{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:331)
[2022-08-13 15:27:35,653] INFO Started @1902ms (org.eclipse.jetty.server.Server:415)
[2022-08-13 15:27:35,670] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 15:27:35,670] INFO REST server listening at http://192.168.0.111:8083/, advertising URL http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:203)
[2022-08-13 15:27:35,670] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 15:27:35,671] INFO REST admin endpoints at http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2022-08-13 15:27:35,671] INFO Advertised URI: http://192.168.0.111:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:355)
[2022-08-13 15:27:35,671] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:44)
[2022-08-13 15:27:35,676] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2022-08-13 15:27:35,677] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 15:27:35,682] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,683] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,683] WARN The configuration 'offset.storage.file.filename' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,683] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,683] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,683] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,683] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:35,683] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:27:35,689] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:27:35,689] INFO Kafka startTimeMs: 1660384655683 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:27:35,699] INFO Kafka cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.connect.util.ConnectUtils:65)
[2022-08-13 15:27:35,701] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:27:35,703] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:27:35,704] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:27:35,704] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:27:35,706] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:27:35,706] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:27:35,706] INFO Kafka startTimeMs: 1660384655706 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:27:35,792] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:27:35,794] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:27:35,806] INFO Kafka Connect standalone worker initialization took 1708ms (org.apache.kafka.connect.cli.ConnectStandalone:99)
[2022-08-13 15:27:35,806] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2022-08-13 15:27:35,807] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:98)
[2022-08-13 15:27:35,807] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:185)
[2022-08-13 15:27:35,808] INFO Starting FileOffsetBackingStore with file C:\kafka\tmp\connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:58)
[2022-08-13 15:27:35,817] INFO Worker started (org.apache.kafka.connect.runtime.Worker:192)
[2022-08-13 15:27:35,817] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:101)
[2022-08-13 15:27:35,817] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:208)
[2022-08-13 15:27:35,842] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:225)
[2022-08-13 15:27:35,889] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2022-08-13 15:27:35,889] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2022-08-13 15:27:35,891] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2022-08-13 15:27:36,189] INFO Started o.e.j.s.ServletContextHandler@221dad51{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:915)
[2022-08-13 15:27:36,189] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2022-08-13 15:27:36,189] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2022-08-13 15:27:36,383] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 15:27:36,389] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:27:36,391] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 15:27:36,398] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 15:27:36,399] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:27:36,399] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:27:36,403] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 15:27:36,405] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 15:27:36,407] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:27:36,408] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:27:36,410] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 15:27:36,412] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 15:27:36,413] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:27:36,415] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 15:27:36,415] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 15:27:36,416] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:27:36,416] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:27:36,416] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 15:27:36,422] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 15:27:36,422] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 15:27:36,425] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:27:36,426] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:27:36,430] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 15:27:36,434] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:27:36,458] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 15:27:36,459] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:27:36,459] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:27:36,459] INFO [connector1|task-0] Kafka startTimeMs: 1660384656459 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:27:36,468] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:27:36,470] INFO Created connector connector1 (org.apache.kafka.connect.cli.ConnectStandalone:109)
[2022-08-13 15:27:36,470] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 15:27:36,471] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,472] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,472] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,472] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,472] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,472] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,472] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,473] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,473] INFO [connector1|task-0]    transforms.unwrap.add.source.fields = table,tls (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,473] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,473] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,473] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,473] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,473] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,473] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,473] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,474] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,474] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,474] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,474] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,474] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,474] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:27:36,527] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1622109, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 15:27:36,556] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 15:27:36,557] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 15:27:36,557] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 15:27:36,559] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:27:36,565] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:27:36,566] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:27:36,566] INFO [connector1|task-0] Kafka startTimeMs: 1660384656565 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:27:36,569] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:27:36,584] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 15:27:36,585] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:27:36,590] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:27:36,614] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:27:36,614] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:27:36,615] INFO [connector1|task-0] Kafka startTimeMs: 1660384656614 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:27:36,619] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:27:36,622] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:27:36,622] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:27:36,623] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:27:36,623] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:27:36,623] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:27:36,624] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:27:36,625] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:27:36,629] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:27:36,630] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:27:36,630] INFO [connector1|task-0] Kafka startTimeMs: 1660384656629 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:27:36,630] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 15:27:36,631] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 15:27:36,637] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:36,642] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:36,641] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:27:36,643] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:36,643] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:36,643] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:36,643] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:36,644] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:27:36,644] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:27:36,644] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:27:36,644] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:27:36,644] INFO [connector1|task-0] Kafka startTimeMs: 1660384656644 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:27:36,665] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:27:36,665] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:27:36,665] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:27:36,666] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:27:36,666] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:27:36,668] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:27:36,669] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:27:36,674] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:27:36,674] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:27:36,674] INFO [connector1|task-0] Kafka startTimeMs: 1660384656674 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:27:36,674] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 15:27:36,675] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:27:36,676] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:27:36,676] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:27:36,677] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:27:36,678] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:27:36,679] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:27:36,680] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:27:36,680] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:27:36,680] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:27:36,680] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:27:36,681] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:27:36,682] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:27:36,687] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:27:36,689] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:27:36,689] INFO [connector1|task-0] Kafka startTimeMs: 1660384656687 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:27:36,692] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:27:36,692] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:27:36,697] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:27:36,699] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:27:36,700] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:27:36,700] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:27:36,700] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:27:36,701] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:27:36,701] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 15:27:36,706] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:27:36,710] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:27:36,710] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:27:36,710] INFO [connector1|task-0] Kafka startTimeMs: 1660384656710 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:27:36,711] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 15:27:36,715] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:27:36,718] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:27:36,722] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 15:27:36,723] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:27:36,731] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:27:36,732] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:27:36,734] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=1, memberId='order_server-dbhistory-4035f7ea-f7ea-413c-8ba7-3420d805101a', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 15:27:36,736] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 1: {order_server-dbhistory-4035f7ea-f7ea-413c-8ba7-3420d805101a=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 15:27:36,740] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=1, memberId='order_server-dbhistory-4035f7ea-f7ea-413c-8ba7-3420d805101a', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 15:27:36,740] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 15:27:36,742] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 15:27:36,749] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 15:27:36,754] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 15:27:37,188] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 15:27:37,189] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-4035f7ea-f7ea-413c-8ba7-3420d805101a sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 15:27:37,190] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:27:37,191] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:27:37,194] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:27:37,195] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:27:37,195] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:27:37,196] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:27:37,196] INFO [connector1|task-0] Finished database history recovery of 33 change(s) in 494 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 15:27:37,210] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 15:27:37,217] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 15:27:37,219] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 15:27:37,243] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 15:27:37,246] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 15:27:37,248] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 15:27:37,248] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 15:27:37,251] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 15:27:37,252] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 15:27:37,256] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 15:27:37,257] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1622109, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1622109, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 15:27:37,260] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 15:27:37,261] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 15:27:37,282] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 15:27:37,282] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 15:27:37,283] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:27:37,287] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:27:37,297] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1622109, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1622109, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 15:27:37,297] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 15:27:37,298] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:27:37,400] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 15:27:44,909] INFO [connector1|task-0] 3 records sent during previous 00:00:08.493, last recorded offset: {transaction_id=null, ts_sec=1660384664, file=SF-CPU-562-bin.000076, pos=1623750, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:27:44,917] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:27:46,472] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:27:56,502] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:28:06,509] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:28:16,524] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:28:26,534] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:28:36,539] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:28:46,545] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:28:56,559] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:29:06,574] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:29:16,584] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:29:26,600] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:29:36,607] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:29:46,612] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:29:56,618] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:30:06,631] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:30:16,632] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:30:26,646] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:30:36,654] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:30:40,741] INFO [connector1|task-0] 3 records sent during previous 00:02:55.832, last recorded offset: {transaction_id=null, ts_sec=1660384840, file=SF-CPU-562-bin.000076, pos=1624960, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:30:46,663] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:30:56,674] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:31:06,688] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:31:16,699] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:31:26,702] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:31:36,710] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:31:46,717] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:31:56,726] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:32:06,730] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:32:16,736] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:32:26,746] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:32:36,752] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:32:46,753] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:32:56,754] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:32:59,258] INFO [connector1|task-0] 3 records sent during previous 00:02:18.517, last recorded offset: {transaction_id=null, ts_sec=1660384979, file=SF-CPU-562-bin.000076, pos=1626170, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:33:06,767] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:33:16,783] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:33:26,790] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:33:36,796] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:33:46,803] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:33:56,808] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:34:06,823] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:34:16,827] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:34:26,840] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:34:36,855] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:34:46,867] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:34:56,876] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:35:06,881] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:35:16,888] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:35:26,893] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:35:36,896] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:35:46,904] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:35:56,912] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:36:06,920] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:36:16,923] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:36:26,931] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:36:36,664] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 15:36:36,945] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:36:40,864] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 15:36:46,958] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:36:56,966] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:37:06,973] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:37:16,986] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:37:26,991] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:37:36,993] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:37:47,002] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:37:57,014] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:38:07,029] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:38:17,034] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:38:27,045] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:38:37,055] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:38:47,064] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:38:57,073] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:39:07,078] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:39:17,092] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:39:25,194] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 15:39:25,196] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:39:25,196] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 15:39:25,197] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 15:39:25,197] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 15:39:25,197] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 15:39:25,197] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 15:39:25,198] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:39:25,198] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:39:25,199] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 15:39:25,199] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 15:39:25,200] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:39:25,200] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:39:25,201] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 15:39:25,666] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:39:25,666] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 15:39:25,713] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 15:39:25,714] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660385050, file=SF-CPU-562-bin.000076, pos=1627638, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 15:39:25,718] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:39:25,720] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:39:25,723] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:39:25,724] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:39:25,724] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:39:25,725] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:39:25,725] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:39:25,727] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:39:25,729] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:39:25,730] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:39:25,730] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:39:25,732] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 15:39:25,735] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 15:39:25,736] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:39:25,736] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 15:39:25,737] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 15:39:25,737] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:39:25,738] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:39:25,738] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 15:39:25,738] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 15:39:25,738] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 15:39:25,739] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:39:25,739] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:39:25,740] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 15:39:25,740] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:39:25,743] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 15:39:25,748] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:39:25,748] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:39:25,748] INFO [connector1|task-0] Kafka startTimeMs: 1660385365748 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:39:25,750] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 15:39:25,751] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,751] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,751] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,751] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,751] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,752] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,752] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,752] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,752] INFO [connector1|task-0]    transformation = or conversion errors (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,752] INFO [connector1|task-0]    transforms.unwrap.add.source.fields = table (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,752] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:39:25,752] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,752] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,753] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,753] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,753] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,753] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,753] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,753] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,753] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,753] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,754] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,754] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,754] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:39:25,762] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1626949, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 15:39:25,769] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 15:39:25,770] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 15:39:25,770] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 15:39:25,771] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:39:25,774] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:39:25,780] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:39:25,780] INFO [connector1|task-0] Kafka startTimeMs: 1660385365774 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:39:25,780] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 15:39:25,781] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:39:25,781] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:39:25,783] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:39:25,786] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:39:25,787] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:39:25,787] INFO [connector1|task-0] Kafka startTimeMs: 1660385365786 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:39:25,790] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:39:25,797] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:39:25,797] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:39:25,798] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:39:25,798] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:39:25,798] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:39:25,801] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:39:25,802] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:39:25,806] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:39:25,811] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:39:25,811] INFO [connector1|task-0] Kafka startTimeMs: 1660385365806 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:39:25,812] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 15:39:25,813] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 15:39:25,817] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:39:25,817] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:39:25,818] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:39:25,818] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:39:25,818] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:39:25,818] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:39:25,818] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:39:25,818] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:39:25,819] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:39:25,819] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:39:25,819] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:39:25,819] INFO [connector1|task-0] Kafka startTimeMs: 1660385365819 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:39:25,825] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:39:25,830] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:39:25,830] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:39:25,830] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:39:25,830] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:39:25,831] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:39:25,832] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:39:25,834] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 15:39:25,835] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:39:25,837] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:39:25,837] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:39:25,837] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:39:25,839] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:39:25,839] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:39:25,840] INFO [connector1|task-0] Kafka startTimeMs: 1660385365839 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:39:25,843] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:39:25,844] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:39:25,844] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:39:25,845] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:39:25,845] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:39:25,846] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:39:25,847] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:39:25,848] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:39:25,852] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:39:25,858] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:39:25,858] INFO [connector1|task-0] Kafka startTimeMs: 1660385365852 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:39:25,862] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:39:25,862] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:39:25,868] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:39:25,870] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:39:25,870] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:39:25,870] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:39:25,871] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:39:25,872] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:39:25,872] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 15:39:25,874] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:39:25,879] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:39:25,879] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:39:25,879] INFO [connector1|task-0] Kafka startTimeMs: 1660385365879 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:39:25,880] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 15:39:25,885] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:39:25,892] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:39:25,897] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 15:39:25,898] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:39:25,904] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:39:25,905] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:39:25,907] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=1, memberId='order_server-dbhistory-ac4e5586-8727-4d07-9d8f-c996c7f314ea', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 15:39:25,907] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 1: {order_server-dbhistory-ac4e5586-8727-4d07-9d8f-c996c7f314ea=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 15:39:25,909] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=1, memberId='order_server-dbhistory-ac4e5586-8727-4d07-9d8f-c996c7f314ea', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 15:39:25,910] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 15:39:25,910] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 15:39:25,912] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 15:39:25,913] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 15:39:26,079] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 15:39:26,079] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-ac4e5586-8727-4d07-9d8f-c996c7f314ea sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 15:39:26,081] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:39:26,083] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:39:26,090] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:39:26,090] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:39:26,090] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:39:26,091] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:39:26,092] INFO [connector1|task-0] Finished database history recovery of 33 change(s) in 219 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 15:39:26,092] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 15:39:26,096] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 15:39:26,098] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 15:39:26,098] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 15:39:26,099] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 15:39:26,100] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 15:39:26,100] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 15:39:26,103] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 15:39:26,107] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 15:39:26,107] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 15:39:26,107] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1626949, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1626949, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 15:39:26,107] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 15:39:26,108] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 15:39:26,110] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 15:39:26,110] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 15:39:26,111] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:39:26,111] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:39:26,117] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1626949, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1626949, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 15:39:26,133] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 15:39:26,133] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:39:26,240] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 15:39:35,756] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:39:45,768] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:39:55,782] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:40:05,787] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:40:15,796] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:40:25,806] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:40:35,812] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:40:45,816] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:40:52,926] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.mysql.MySqlConnector:100)
[2022-08-13 15:40:52,927] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:40:52,928] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:376)
[2022-08-13 15:40:52,928] INFO [connector1|worker] Stopping connector connector1 (org.apache.kafka.connect.runtime.Worker:376)
[2022-08-13 15:40:52,928] INFO [connector1|worker] Scheduled shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:248)
[2022-08-13 15:40:52,929] INFO [connector1|worker] Completed shutdown for WorkerConnector{id=connector1} (org.apache.kafka.connect.runtime.WorkerConnector:268)
[2022-08-13 15:40:52,929] INFO [connector1|worker] Creating connector connector1 of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:265)
[2022-08-13 15:40:52,929] INFO [connector1|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:40:52,930] INFO [connector1|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:40:52,930] INFO [connector1|worker] Instantiated connector connector1 with version 1.9.3.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:275)
[2022-08-13 15:40:52,930] INFO [connector1|worker] Finished creating connector connector1 (org.apache.kafka.connect.runtime.Worker:300)
[2022-08-13 15:40:52,931] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:40:52,931] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:40:52,931] INFO [connector1|task-0] Stopping task connector1-0 (org.apache.kafka.connect.runtime.Worker:823)
[2022-08-13 15:40:53,048] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:40:53,048] INFO [connector1|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:238)
[2022-08-13 15:40:53,095] INFO [connector1|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:175)
[2022-08-13 15:40:53,095] INFO [connector1|task-0] Stopped reading binlog after 0 events, last recorded offset: {transaction_id=null, ts_sec=1660385050, file=SF-CPU-562-bin.000076, pos=1627638, server_id=1, event=1} (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1205)
[2022-08-13 15:40:53,098] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:40:53,099] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:40:53,100] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:40:53,101] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:40:53,101] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:40:53,101] INFO [connector1|task-0] App info kafka.producer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:40:53,102] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1228)
[2022-08-13 15:40:53,104] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:40:53,104] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:40:53,104] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:40:53,105] INFO [connector1|task-0] App info kafka.producer for connector-producer-connector1-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:40:53,106] INFO [connector1|task-0] Creating task connector1-0 (org.apache.kafka.connect.runtime.Worker:499)
[2022-08-13 15:40:53,106] INFO [connector1|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:376)
[2022-08-13 15:40:53,108] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:40:53,108] INFO [connector1|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:376)
[2022-08-13 15:40:53,108] INFO [connector1|task-0] Instantiated task connector1-0 with version 1.9.3.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:514)
[2022-08-13 15:40:53,109] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:40:53,109] INFO [connector1|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:376)
[2022-08-13 15:40:53,110] INFO [connector1|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:529)
[2022-08-13 15:40:53,110] INFO [connector1|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task connector1-0 using the connector config (org.apache.kafka.connect.runtime.Worker:535)
[2022-08-13 15:40:53,110] INFO [connector1|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task connector1-0 using the worker config (org.apache.kafka.connect.runtime.Worker:540)
[2022-08-13 15:40:53,111] INFO [connector1|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:376)
[2022-08-13 15:40:53,120] INFO [connector1|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = connector1
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.headers = []
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = 
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:376)
[2022-08-13 15:40:53,121] INFO [connector1|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState} (org.apache.kafka.connect.runtime.Worker:594)
[2022-08-13 15:40:53,121] INFO [connector1|task-0] ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-connector1-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:40:53,127] WARN [connector1|task-0] The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:384)
[2022-08-13 15:40:53,127] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:40:53,127] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:40:53,127] INFO [connector1|task-0] Kafka startTimeMs: 1660385453127 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:40:53,129] INFO [connector1|task-0] Starting MySqlConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:124)
[2022-08-13 15:40:53,130] INFO [connector1|task-0]    connector.class = io.debezium.connector.mysql.MySqlConnector (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,130] INFO [connector1|task-0]    database.user = root (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,130] INFO [connector1|task-0]    database.dbname = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,130] INFO [connector1|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,131] INFO [connector1|task-0]    database.history.kafka.bootstrap.servers = localhost:9092 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,131] INFO [connector1|task-0]    database.history.kafka.topic = demo (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,131] INFO [connector1|task-0]    transforms = unwrap (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,131] INFO [connector1|task-0]    database.server.name = order_server (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,131] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:40:53,131] INFO [connector1|task-0]    transforms.unwrap.add.source.fields = table (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,132] INFO [connector1|task-0]    database.port = 3306 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,132] INFO [connector1|task-0]    include.schema.changes = true (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,132] INFO [connector1|task-0]    offset.storage.file.filename = C:/kafka/tmp/connect.offsets (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,132] INFO [connector1|task-0]    task.class = io.debezium.connector.mysql.MySqlConnectorTask (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,132] INFO [connector1|task-0]    database.hostname = localhost (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,133] INFO [connector1|task-0]    database.password = ******** (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,133] INFO [connector1|task-0]    name = connector1 (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,133] INFO [connector1|task-0]    transforms.unwrap.type = io.debezium.transforms.ExtractNewRecordState (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,133] INFO [connector1|task-0]    table.include.list = orders.outbox (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,133] INFO [connector1|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,134] INFO [connector1|task-0]    database.database.whitelist = test (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,134] INFO [connector1|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,134] INFO [connector1|task-0]    database.include.list = orders (io.debezium.connector.common.BaseSourceTask:126)
[2022-08-13 15:40:53,147] INFO [connector1|task-0] Found previous partition offset MySqlPartition [sourcePartition={server=order_server}]: {transaction_id=null, file=SF-CPU-562-bin.000076, pos=1626949, row=1, event=2} (io.debezium.connector.common.BaseSourceTask:313)
[2022-08-13 15:40:53,156] INFO [connector1|task-0] KafkaDatabaseHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=order_server-dbhistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=order_server-dbhistory} (io.debezium.relational.history.KafkaDatabaseHistory:243)
[2022-08-13 15:40:53,156] INFO [connector1|task-0] KafkaDatabaseHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=order_server-dbhistory, linger.ms=0} (io.debezium.relational.history.KafkaDatabaseHistory:244)
[2022-08-13 15:40:53,156] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = db-history-config-check (io.debezium.util.Threads:270)
[2022-08-13 15:40:53,157] INFO [connector1|task-0] ProducerConfig values: 
	acks = 1
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:376)
[2022-08-13 15:40:53,163] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:40:53,167] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:40:53,167] INFO [connector1|task-0] Kafka startTimeMs: 1660385453163 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:40:53,167] INFO [connector1|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:95)
[2022-08-13 15:40:53,168] INFO [connector1|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:956)
[2022-08-13 15:40:53,169] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:40:53,170] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:40:53,173] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:40:53,173] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:40:53,174] INFO [connector1|task-0] Kafka startTimeMs: 1660385453173 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:40:53,176] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:40:53,178] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:40:53,178] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:40:53,178] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:40:53,178] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:40:53,178] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:40:53,182] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:40:53,183] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:40:53,186] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:40:53,187] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:40:53,187] INFO [connector1|task-0] Kafka startTimeMs: 1660385453186 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:40:53,187] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-db-history-config-check (io.debezium.util.Threads:287)
[2022-08-13 15:40:53,188] INFO [connector1|task-0] AdminClientConfig values: 
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:376)
[2022-08-13 15:40:53,190] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:40:53,191] WARN [connector1|task-0] The configuration 'value.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:40:53,196] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:40:53,196] WARN [connector1|task-0] The configuration 'acks' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:40:53,197] WARN [connector1|task-0] The configuration 'batch.size' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:40:53,197] WARN [connector1|task-0] The configuration 'max.block.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:40:53,197] WARN [connector1|task-0] The configuration 'buffer.memory' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:40:53,197] WARN [connector1|task-0] The configuration 'key.serializer' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:40:53,197] WARN [connector1|task-0] The configuration 'linger.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:384)
[2022-08-13 15:40:53,197] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:40:53,197] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:40:53,197] INFO [connector1|task-0] Kafka startTimeMs: 1660385453197 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:40:53,201] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:40:53,201] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:40:53,202] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:40:53,202] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:40:53,203] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:40:53,205] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:40:53,206] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:40:53,212] INFO [connector1|task-0] Database history topic 'demo' has correct settings (io.debezium.relational.history.KafkaDatabaseHistory:468)
[2022-08-13 15:40:53,213] INFO [connector1|task-0] App info kafka.admin.client for order_server-dbhistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:40:53,215] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:40:53,215] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:40:53,216] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:40:53,217] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:40:53,217] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:40:53,217] INFO [connector1|task-0] Kafka startTimeMs: 1660385453217 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:40:53,222] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:40:53,231] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:40:53,231] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:40:53,232] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:40:53,232] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:40:53,232] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:40:53,233] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:40:53,233] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:40:53,238] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:40:53,239] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:40:53,240] INFO [connector1|task-0] Kafka startTimeMs: 1660385453238 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:40:53,243] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:40:53,244] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:40:53,247] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:40:53,248] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:40:53,248] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:40:53,248] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:40:53,248] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:40:53,249] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:40:53,249] INFO [connector1|task-0] Started database history recovery (io.debezium.relational.history.DatabaseHistoryMetrics:108)
[2022-08-13 15:40:53,250] INFO [connector1|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = order_server-dbhistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = order_server-dbhistory
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:376)
[2022-08-13 15:40:53,252] INFO [connector1|task-0] Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser:119)
[2022-08-13 15:40:53,253] INFO [connector1|task-0] Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser:120)
[2022-08-13 15:40:53,259] INFO [connector1|task-0] Kafka startTimeMs: 1660385453252 (org.apache.kafka.common.utils.AppInfoParser:121)
[2022-08-13 15:40:53,259] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Subscribed to topic(s): demo (org.apache.kafka.clients.consumer.KafkaConsumer:966)
[2022-08-13 15:40:53,261] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting the last seen epoch of partition demo-0 to 0 since the associated topicId changed from null to fVtJG6OrTcmb2oaPpEj5Xw (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:40:53,262] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Cluster ID: 4wg6huU-T3i2wdraQxDkuw (org.apache.kafka.clients.Metadata:287)
[2022-08-13 15:40:53,264] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:853)
[2022-08-13 15:40:53,265] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:40:53,268] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: need to re-join with the given member-id (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:40:53,283] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:535)
[2022-08-13 15:40:53,285] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully joined group with generation Generation{generationId=3, memberId='order_server-dbhistory-4cc9319b-4849-4614-bf1a-ff21acf307ab', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:595)
[2022-08-13 15:40:53,286] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Finished assignment for group at generation 3: {order_server-dbhistory-4cc9319b-4849-4614-bf1a-ff21acf307ab=Assignment(partitions=[demo-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:652)
[2022-08-13 15:40:53,288] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Successfully synced group in generation Generation{generationId=3, memberId='order_server-dbhistory-4cc9319b-4849-4614-bf1a-ff21acf307ab', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:761)
[2022-08-13 15:40:53,288] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Notifying assignor about the new Assignment(partitions=[demo-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:279)
[2022-08-13 15:40:53,288] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Adding newly assigned partitions: demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:291)
[2022-08-13 15:40:53,289] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Found no committed offset for partition demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1388)
[2022-08-13 15:40:53,291] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting offset for partition demo-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:398)
[2022-08-13 15:40:53,329] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Revoke previously assigned partitions demo-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:310)
[2022-08-13 15:40:53,329] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Member order_server-dbhistory-4cc9319b-4849-4614-bf1a-ff21acf307ab sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1060)
[2022-08-13 15:40:53,329] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Resetting generation due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:972)
[2022-08-13 15:40:53,329] INFO [connector1|task-0] [Consumer clientId=order_server-dbhistory, groupId=order_server-dbhistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1000)
[2022-08-13 15:40:53,334] INFO [connector1|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:659)
[2022-08-13 15:40:53,334] INFO [connector1|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:663)
[2022-08-13 15:40:53,334] INFO [connector1|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:669)
[2022-08-13 15:40:53,335] INFO [connector1|task-0] App info kafka.consumer for order_server-dbhistory unregistered (org.apache.kafka.common.utils.AppInfoParser:83)
[2022-08-13 15:40:53,336] INFO [connector1|task-0] Finished database history recovery of 33 change(s) in 86 ms (io.debezium.relational.history.DatabaseHistoryMetrics:114)
[2022-08-13 15:40:53,336] INFO [connector1|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:109)
[2022-08-13 15:40:53,340] INFO [connector1|task-0] Get all known binlogs from MySQL (io.debezium.connector.mysql.MySqlConnection:397)
[2022-08-13 15:40:53,342] INFO [connector1|task-0] MySQL has the binlog file 'SF-CPU-562-bin.000076' required by the connector (io.debezium.connector.mysql.MySqlConnectorTask:317)
[2022-08-13 15:40:53,343] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2022-08-13 15:40:53,343] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-change-event-source-coordinator (io.debezium.util.Threads:287)
[2022-08-13 15:40:53,343] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:226)
[2022-08-13 15:40:53,343] INFO [connector1|task-0] WorkerSourceTask{id=connector1-0} Executing source task (org.apache.kafka.connect.runtime.WorkerSourceTask:232)
[2022-08-13 15:40:53,345] INFO [connector1|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:103)
[2022-08-13 15:40:53,349] INFO [connector1|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:106)
[2022-08-13 15:40:53,349] INFO [connector1|task-0] A previous offset indicating a completed snapshot has been found. Neither schema nor data will be snapshotted. (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:82)
[2022-08-13 15:40:53,349] INFO [connector1|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1626949, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1626949, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:156)
[2022-08-13 15:40:53,349] INFO [connector1|task-0] Requested thread factory for connector MySqlConnector, id = order_server named = binlog-client (io.debezium.util.Threads:270)
[2022-08-13 15:40:53,349] INFO [connector1|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:173)
[2022-08-13 15:40:53,352] INFO [connector1|task-0] Skip 2 events on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:923)
[2022-08-13 15:40:53,353] INFO [connector1|task-0] Skip 1 rows on streaming start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:927)
[2022-08-13 15:40:53,353] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:40:53,355] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:40:53,360] INFO [connector1|task-0] Connected to MySQL binlog at localhost:3306, starting at MySqlOffsetContext [sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=SourceInfo [currentGtid=null, currentBinlogFilename=SF-CPU-562-bin.000076, currentBinlogPosition=1626949, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery=null, tableIds=[], databaseName=null], snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet=null, currentGtidSet=null, restartBinlogFilename=SF-CPU-562-bin.000076, restartBinlogPosition=1626949, restartRowsToSkip=1, restartEventsToSkip=2, currentEventLengthInBytes=0, inTransaction=false, transactionId=null, incrementalSnapshotContext =IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]] (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:1220)
[2022-08-13 15:40:53,360] INFO [connector1|task-0] Waiting for keepalive thread to start (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:944)
[2022-08-13 15:40:53,360] INFO [connector1|task-0] Creating thread debezium-mysqlconnector-order_server-binlog-client (io.debezium.util.Threads:287)
[2022-08-13 15:40:53,470] INFO [connector1|task-0] Keepalive thread is running (io.debezium.connector.mysql.MySqlStreamingChangeEventSource:951)
[2022-08-13 15:41:03,144] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:41:09,190] INFO [connector1|task-0] 3 records sent during previous 00:00:16.081, last recorded offset: {transaction_id=null, ts_sec=1660385468, file=SF-CPU-562-bin.000076, pos=1628473, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:41:09,194] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Resetting the last seen epoch of partition order_server.orders.outbox-0 to 0 since the associated topicId changed from null to cg1yzq_lQtOjiNWGWqaZ4g (org.apache.kafka.clients.Metadata:402)
[2022-08-13 15:41:13,160] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:41:23,166] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:41:33,179] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:41:43,185] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:41:53,200] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:42:03,204] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:42:13,214] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:42:23,229] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:42:33,241] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:42:43,253] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:42:53,265] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:43:03,269] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:43:13,272] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:43:23,284] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:43:33,301] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:43:43,310] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:43:53,317] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:44:03,324] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:44:13,329] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:44:23,334] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:44:33,336] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:44:43,345] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:44:53,352] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:45:03,361] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:45:13,363] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:45:23,364] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:45:33,371] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:45:43,379] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:45:53,382] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:46:03,392] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:46:13,403] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:46:23,410] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:46:33,424] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:46:43,429] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:46:53,435] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:47:03,437] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:47:13,450] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:47:22,027] INFO [connector1|task-0] 1 records sent during previous 00:06:12.837, last recorded offset: {transaction_id=null, ts_sec=1660385841, file=SF-CPU-562-bin.000076, pos=1629213, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:47:23,474] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:47:33,487] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:47:43,500] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:47:53,514] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:48:03,518] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:48:13,523] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:48:23,528] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:48:33,535] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:48:43,540] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:48:53,547] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:49:03,551] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:49:13,554] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:49:23,569] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:49:33,571] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:49:43,584] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:49:53,251] INFO [connector1|task-0] [Producer clientId=order_server-dbhistory] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 15:49:53,593] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:50:03,601] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:50:13,617] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:50:22,085] INFO [connector1|task-0] [Producer clientId=connector-producer-connector1-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:935)
[2022-08-13 15:50:23,626] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:50:33,638] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:50:43,643] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:50:53,649] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:51:03,658] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:51:13,665] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:51:23,675] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:51:33,682] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:51:43,695] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:51:53,698] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:52:03,704] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:52:13,706] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:52:23,709] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:52:33,718] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:52:43,725] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:52:53,728] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:53:03,734] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:53:13,744] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:53:23,755] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:53:25,673] INFO [connector1|task-0] 1 records sent during previous 00:06:03.646, last recorded offset: {transaction_id=null, ts_sec=1660386205, file=SF-CPU-562-bin.000076, pos=1629969, row=1, server_id=1, event=2} (io.debezium.connector.common.BaseSourceTask:182)
[2022-08-13 15:53:33,760] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:53:43,774] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:53:53,777] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:54:03,786] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:54:13,791] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:54:23,801] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:54:33,812] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:54:43,824] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:54:53,827] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:55:03,842] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:55:13,852] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:55:23,865] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:55:33,877] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:55:43,882] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:55:53,884] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:56:03,888] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:56:13,905] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:56:23,912] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:56:33,926] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:56:43,937] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:56:53,944] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:57:03,951] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:57:13,961] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:57:23,970] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:57:33,979] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:57:43,990] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:57:53,997] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:58:04,004] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:58:14,017] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:58:24,030] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:58:34,034] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:58:44,047] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:58:54,054] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:59:04,054] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:59:14,062] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:59:24,065] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:59:34,071] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:59:44,080] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
[2022-08-13 15:59:54,090] INFO [connector1|task-0|offsets] WorkerSourceTask{id=connector1-0} Either no records were produced by the task since the last offset commit, or every record has been filtered out by a transformation or dropped due to transformation or conversion errors. (org.apache.kafka.connect.runtime.WorkerSourceTask:484)
